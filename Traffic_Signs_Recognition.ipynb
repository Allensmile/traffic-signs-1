{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier\n",
    "\n",
    "In this notebook, a template is provided for you to implement your functionality in stages which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission, if necessary. Sections that begin with **'Implementation'** in the header indicate where you should begin your implementation for your project. Note that some sections of implementation are optional, and will be marked with **'Optional'** in the header.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Exploration\n",
    "\n",
    "Visualize the German Traffic Signs Dataset. This is open ended, some suggestions include: plotting traffic signs images, plotting the count of each sign, etc. Be creative!\n",
    "\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- features -> the images pixel values, (width, height, channels)\n",
    "- labels -> the label of the traffic sign\n",
    "- sizes -> the original width and height of the image, (width, height)\n",
    "- coords -> coordinates of a bounding box around the sign in the image, (x1, y1, x2, y2). Based the original image (not the resized version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg', warn = False)\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_pickled_data(file, columns):\n",
    "    \"\"\"\n",
    "    Loads pickled training and test data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file    : \n",
    "              Name of the pickle file.\n",
    "    columns : list of strings, optional, defaults to `['features', 'labels']`\n",
    "              List of columns in pickled data we're interested in.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of datasets for given columns.    \n",
    "    \"\"\"\n",
    "\n",
    "    with open(file, mode='rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    return tuple(map(lambda c: dataset[c], columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.io.parsers import read_csv\n",
    "\n",
    "signnames = read_csv(\"signnames.csv\").values[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset_file = \"traffic-signs-data/train.p\"\n",
    "test_dataset_file = \"traffic-signs-data/test.p\"\n",
    "train_balanced_dataset_file = \"traffic-signs-data/train_balanced.p\"\n",
    "\n",
    "X_train, y_train, sizes, positions = load_pickled_data(train_dataset_file, ['features', 'labels', 'sizes', 'coords'])\n",
    "X_test, y_test = load_pickled_data(test_dataset_file, ['features', 'labels'])\n",
    "\n",
    "n_train = y_train.shape[0]\n",
    "n_test = y_test.shape[0]\n",
    "image_shape = X_train[0].shape\n",
    "image_size = image_shape[0]\n",
    "sign_classes, class_indices, class_counts = np.unique(y_train, return_index = True, return_counts = True)\n",
    "n_classes = class_counts.shape[0]\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original photos\n",
    "\n",
    "Let's first explore data about original photos. Who knows — maybe we can spot some data correlations with signs positions in the photos or positions across specific sign classes.\n",
    "\n",
    "Let's first plot histograms of original photos' sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "width_fraction = (positions[:, 2] - positions[:, 0]) / sizes[:, 0]\n",
    "height_fraction = (positions[:, 3] - positions[:, 1]) / sizes[:, 1]\n",
    "sizes_figure = pyplot.figure(figsize = (10, 4))\n",
    "\n",
    "axis = sizes_figure.add_subplot(1, 2, 1)\n",
    "axis.hist(sizes[:, 0])\n",
    "pyplot.title('Original photos\\'s widths')\n",
    "pyplot.xlabel('Width')\n",
    "pyplot.ylabel('Photos')\n",
    "\n",
    "axis = sizes_figure.add_subplot(1, 2, 2)\n",
    "axis.hist(sizes[:, 1])\n",
    "pyplot.title('Original photos\\'s heights')\n",
    "pyplot.xlabel('Height')\n",
    "pyplot.ylabel('Photos')\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't look too promising — apparently photos are of all different sizes, majority ranging from ~30 to ~150 px both width and height. Actually both histograms looks surprisingly similar, I bet aspect ratios of all photos are close to 1. Let's check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sizes_figure = pyplot.figure()\n",
    "axis = sizes_figure.add_subplot(1, 1, 1)\n",
    "axis.hist(sizes[:, 0]/sizes[:, 1])\n",
    "pyplot.title('Photos\\' aspect ratios distribution')\n",
    "pyplot.xlabel('Aspect ratio')\n",
    "pyplot.ylabel('Photos')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go! Vast majority of original photos had aspect ratio ranging from 0.9 to 1.2, e.g. almost all photos were more or less square.\n",
    "\n",
    "Let's check how much area of the original image did signs occupy? In other words, to which extent did we have to crop the original photo to get the sign out of it. Again, let's check for both width and height by calculating sign dimensions as fractions of original photo dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "width_fraction = (positions[:, 2] - positions[:, 0]) / sizes[:, 0]\n",
    "height_fraction = (positions[:, 3] - positions[:, 1]) / sizes[:, 1]\n",
    "\n",
    "sizes_figure = pyplot.figure(figsize = (10, 4))\n",
    "\n",
    "axis = sizes_figure.add_subplot(1, 2, 1)\n",
    "axis.hist(width_fraction)\n",
    "pyplot.title('Sign width / original photo width')\n",
    "pyplot.xlabel('Width fraction')\n",
    "pyplot.ylabel('Photos')\n",
    "\n",
    "axis = sizes_figure.add_subplot(1, 2, 2)\n",
    "axis.hist(height_fraction)\n",
    "pyplot.title('Sign height / original photo height')\n",
    "pyplot.xlabel('Height fraction')\n",
    "pyplot.ylabel('Photos')\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def print_stats(array, label):\n",
    "    _, minmax, mean, variance, _, _ = stats.describe(array)\n",
    "    margin = int((40 - len(label)) / 2)\n",
    "    for i in range(margin * 2): \n",
    "        if i == margin: print(label.upper(), end=\"\")\n",
    "        print(\"=\", end=\"\")    \n",
    "    print()\n",
    "\n",
    "    print(\"  %-*s %s\" % (15, \"Min:\", str(minmax[0])))\n",
    "    print(\"  %-*s %s\" % (15, \"Max:\", str(minmax[1])))\n",
    "    print(\"  %-*s %s\" % (15, \"Mean:\", str(mean)))\n",
    "    print(\"  %-*s %s\" % (15, \"Variance:\", str(variance)))\n",
    "    \n",
    "    print()\n",
    "    \n",
    "print_stats(width_fraction, \" sign_width / photo_width \")\n",
    "print_stats(height_fraction, \" sign_height / photo_height \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really promising either. All photos had almost nothing but signs in them, for both width and height signs occupied at least half of the dimension, and for majority of samples signs occupied around 75% of the photo.\n",
    "\n",
    "Now, if we found any uneven distributions in the dataset I would be tempted to try and analyze it on per-class basis. Although for now data looks pretty homogeneous and I doubt there will be any meaningful patterns related to specific classes based on the original samples' images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample images\n",
    "\n",
    "Ok, let's check out the sign samples. We will go through all dataset classes, noting number of samples and plotting 10 random images representing each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "col_width = max(len(name) for name in signnames)\n",
    "\n",
    "for c, c_index, c_count in zip(sign_classes, class_indices, class_counts):\n",
    "    print(\"Class %i: %-*s  %s samples\" % (c, col_width, signnames[c], str(c_count)))\n",
    "    fig = pyplot.figure(figsize = (6, 1))\n",
    "    fig.subplots_adjust(left = 0, right = 1, bottom = 0, top = 1, hspace = 0.05, wspace = 0.05)\n",
    "    random_indices = random.sample(range(c_index, c_index + c_count), 10)\n",
    "    for i in range(10):\n",
    "        axis = fig.add_subplot(1, 10, i + 1, xticks=[], yticks=[])\n",
    "        axis.imshow(X_train[random_indices[i]])\n",
    "    pyplot.show()\n",
    "    print(\"--------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bad news**: Some classes are highly underrepresented. Overall amount of data is disappointing: some classes have as little as 210 samples, this won't be enough for most of the models to generalise well.\n",
    "\n",
    "**Good news**: There is a room for data augmentation.\n",
    "\n",
    "Images with some signs are horizontally symmetrical (like **Bumpy road** or **Ahead only**), and can be simply flipped horizontally, thus allowing us to get twice as much data for these classes. \n",
    "\n",
    "Other signs come in kind of interchageable pairs, like **Keep right** and **Keep left**: those signs can be flipped and assigned to a paired class. In some cases (like **Keep right** and **Keep left**) we increase the number of samples for **Keep left** from 300 to 2370!\n",
    "\n",
    "CNNs have built-in invariance to small translations, scaling and rotations. The training set doesn't seem to contain those deformations, so we will add those in our data augmentation step as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "Design and implement a deep learning model that learns to recognize traffic signs. Train and test your model on the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "\n",
    "- Your model can be derived from a deep feedforward net or a deep convolutional network.\n",
    "- Play around preprocessing techniques (normalization, rgb to grayscale, etc)\n",
    "- Number of examples per label (some have more than others).\n",
    "- Generate fake data.\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf). It's not required to be familiar with the approach used in the paper but, it's good practice to try to read papers like these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "num_classes = 43\n",
    "\n",
    "def preprocess_dataset(X, y = None):\n",
    "    \"\"\"\n",
    "    Performs feature scaling, one-hot encoding of labels and shuffles the data if labels are provided.\n",
    "    Assumes original dataset is sorted by labels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X                : ndarray\n",
    "                       Dataset array containing feature examples.\n",
    "    y                : ndarray, optional, defaults to `None`\n",
    "                       Dataset labels in index form.\n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of X and y.    \n",
    "    \"\"\"\n",
    "        \n",
    "    # Convert to grayscale, e.g. single channel Y\n",
    "    X = 0.2125 * X[:, :, :, 0] + 0.7154 * X[:, :, :, 1] + 0.0721 * X[:, :, :, 2]\n",
    "    # Scale features to be in [0, 1]\n",
    "    X = (X / 255.).astype(np.float32)\n",
    "        \n",
    "    if y is not None:  \n",
    "        # Convert to one-hot encoding. Convert back with `y = y.nonzero()[1]`\n",
    "        y = np.eye(num_classes)[y]\n",
    "        X, y = shuffle(X, y, random_state = 42)\n",
    "\n",
    "    # Add a single grayscale channel\n",
    "    X = X.reshape(X.shape + (1,)) \n",
    "    return X, y\n",
    "\n",
    "def class_name(one_hot):\n",
    "    return signnames[one_hot.nonzero()[0][0]]\n",
    "\n",
    "def load_and_process_data(pickled_data_file):\n",
    "    \"\"\"\n",
    "    Loads pickled data and preprocesses images and labels by scaling features, \n",
    "    shuffling the data and applying one-hot encoding to labels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pickled_data_file  : string\n",
    "                         Pickled data file name.\n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of X and y containing preloaded and preprocessed features and labels respectively.    \n",
    "    \"\"\"\n",
    "    X, y = load_pickled_data(pickled_data_file, columns = ['features', 'labels'])\n",
    "    X, y = preprocess_dataset(X, y)\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/bin/anaconda3/lib/python3.5/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n",
      "/home/alex/bin/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from nolearn.lasagne import BatchIterator\n",
    "from skimage.transform import rotate\n",
    "from skimage.transform import warp\n",
    "from skimage.transform import ProjectiveTransform\n",
    "from skimage.util import random_noise\n",
    "from skimage.exposure import adjust_gamma\n",
    "from skimage.filters import gaussian\n",
    "import random\n",
    "\n",
    "class AugmentedSignsBatchIterator(BatchIterator):\n",
    "    \"\"\"\n",
    "    Iterates over dataset in batches. \n",
    "    Allows images augmentation by randomly rotating, applying projection, \n",
    "    adjusting gamma, blurring, adding noize and flipping horizontally.\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, batch_size, shuffle = False, seed = 42, p = 0.5, intensity = 0.5):\n",
    "        \"\"\"\n",
    "        Initialises an instance with usual iterating settings, as well as data augmentation coverage\n",
    "        and augmentation intensity.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size:\n",
    "                    Size of the iteration batch.\n",
    "        shuffle   :\n",
    "                    Flag indicating if we need to shuffle the data.\n",
    "        seed      :\n",
    "                    Random seed.\n",
    "        p         :\n",
    "                    Probability of augmenting a single example, should be in a range of [0, 1] .\n",
    "                    Defines data augmentation coverage.\n",
    "        intensity :\n",
    "                    Augmentation intensity, should be in a [0, 1] range.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        New batch iterator instance.\n",
    "        \"\"\"\n",
    "        super(AugmentedSignsBatchIterator, self).__init__(batch_size, shuffle, seed)\n",
    "        self.p = p\n",
    "        self.intensity = intensity\n",
    "\n",
    "    def transform(self, Xb, yb):\n",
    "        \"\"\"\n",
    "        Applies a pipeline of randomised transformations for data augmentation.\n",
    "        \"\"\"\n",
    "        Xb, yb = super(AugmentedSignsBatchIterator, self).transform(\n",
    "            Xb if yb is None else Xb.copy(), \n",
    "            yb\n",
    "        )\n",
    "        \n",
    "        if yb is not None:\n",
    "            batch_size = Xb.shape[0]\n",
    "            image_size = Xb.shape[1]\n",
    "            \n",
    "            Xb = self.rotate(Xb, batch_size)\n",
    "            Xb = self.apply_projection_transform(Xb, batch_size, image_size)\n",
    "            Xb = self.adjust_gamma(Xb, batch_size)\n",
    "            Xb = self.blur(Xb, batch_size)\n",
    "            #Xb = self.add_noise(Xb, batch_size)\n",
    "\n",
    "        return Xb, yb\n",
    "        \n",
    "    def rotate(self, Xb, batch_size):\n",
    "        \"\"\"\n",
    "        Applies random rotation in a defined degrees range to a random subset of images. \n",
    "        Range itself is subject to scaling depending on augmentation intensity.\n",
    "        \"\"\"\n",
    "        for i in np.random.choice(batch_size, int(batch_size * self.p), replace = False):\n",
    "            delta = 30. * self.intensity # scale by self.intensity\n",
    "            Xb[i] = rotate(Xb[i], random.uniform(-delta, delta), mode = 'edge')\n",
    "        return Xb \n",
    "    \n",
    "    def add_noise(self, Xb, batch_size):\n",
    "        \"\"\"\n",
    "        Adds random noise to a random subset of images.\n",
    "        \"\"\"\n",
    "        modes = [\"gaussian\", \"localvar\", \"poisson\", \"speckle\"] # Excluding \"salt\", \"pepper\", \"s&p\" for now\n",
    "        for i in np.random.choice(batch_size, int(batch_size * self.p), replace = False):\n",
    "            Xb[i] = random_noise(Xb[i], mode = random.choice(modes))\n",
    "        return Xb\n",
    "    \n",
    "    def adjust_gamma(self, Xb, batch_size):\n",
    "        \"\"\"\n",
    "        Applies gamma adjustment to a random subset of images, updating gamma with a random value in the defined range. \n",
    "        Range itself is subject to scaling depending on augmentation intensity.\n",
    "        \"\"\"\n",
    "        for i in np.random.choice(batch_size, int(batch_size * self.p), replace = False):\n",
    "            delta = 0.8 * self.intensity\n",
    "            Xb[i] = adjust_gamma(Xb[i], gamma = random.uniform(1. - delta, 1. + delta))\n",
    "        return Xb        \n",
    "\n",
    "    def blur(self, Xb, batch_size):\n",
    "        \"\"\"\n",
    "        Applies gaussian blur with a random sigma in the defined range to a random subset of images.\n",
    "        Range itself is subject to scaling depending on augmentation intensity.\n",
    "        \"\"\"\n",
    "        for i in np.random.choice(batch_size, int(batch_size * self.p), replace = False):\n",
    "            max_sigma = 1.5 * self.intensity\n",
    "            Xb[i] = gaussian(Xb[i], sigma = random.uniform(0., max_sigma), multichannel = Xb.shape[3] == 3)\n",
    "        return Xb   \n",
    "    \n",
    "    def apply_projection_transform(self, Xb, batch_size, image_size):\n",
    "        \"\"\"\n",
    "        Applies projection transform to a random subset of images. Projection margins are randomised in a range\n",
    "        depending on the size of the image. Range itself is subject to scaling depending on augmentation intensity.\n",
    "        \"\"\"\n",
    "        d = image_size * 0.25 * self.intensity\n",
    "        for i in np.random.choice(batch_size, int(batch_size * self.p), replace = False):        \n",
    "            tl_top = random.uniform(-d, d)     # Top left corner, top margin\n",
    "            tl_left = random.uniform(-d, d)    # Top left corner, left margin\n",
    "            bl_bottom = random.uniform(-d, d)  # Bottom left corner, bottom margin\n",
    "            bl_left = random.uniform(-d, d)    # Bottom left corner, left margin\n",
    "            tr_top = random.uniform(-d, d)     # Top right corner, top margin\n",
    "            tr_right = random.uniform(-d, d)   # Top right corner, right margin\n",
    "            br_bottom = random.uniform(-d, d)  # Bottom right corner, bottom margin\n",
    "            br_right = random.uniform(-d, d)   # Bottom right corner, right margin\n",
    "\n",
    "            transform = ProjectiveTransform()\n",
    "            transform.estimate(np.array((\n",
    "                    (tl_left, tl_top),\n",
    "                    (bl_left, image_size - bl_bottom),\n",
    "                    (image_size - br_right, image_size - br_bottom),\n",
    "                    (image_size - tr_right, tr_top)\n",
    "                )), np.array((\n",
    "                    (0, 0),\n",
    "                    (0, image_size),\n",
    "                    (image_size, image_size),\n",
    "                    (image_size, 0)\n",
    "                )))\n",
    "            Xb[i] = warp(Xb[i], transform, output_shape=(image_size, image_size), mode = 'edge')\n",
    "\n",
    "        return Xb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of functions for preprocessing our dataset. We may only need to call them once, then pickle resulting balanced dataset and keep using that pickle from then on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extend_flipping_horizontally(X, y):\n",
    "    \"\"\"\n",
    "    Extends existing images dataset by flipping images of some classes. As some images would still belong\n",
    "    to same class after horizontal flipping we extend such classes with flipped images. Images of other\n",
    "    classes would toggle between two classes when flipped, so for those we extend existing datasets as\n",
    "    well.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X       : ndarray\n",
    "              Dataset array containing feature examples.\n",
    "    y       : ndarray, optional, defaults to `None`\n",
    "              Dataset labels in index form.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of X and y.    \n",
    "    \"\"\"\n",
    "    # Classes of signs that, when flipped horizontally, should still be classified as the same class\n",
    "    self_flippable = np.array([11, 12, 13, 15, 17, 18, 22, 26, 30, 35])\n",
    "    # Classes of signs that, when flipped horizontally, should still be classified as some other class\n",
    "    cross_flippable = np.array([\n",
    "        [19, 20], \n",
    "        [33, 34], \n",
    "        [36, 37], \n",
    "        [38, 39],\n",
    "        [20, 19], \n",
    "        [34, 33], \n",
    "        [37, 36], \n",
    "        [39, 38],   \n",
    "    ])\n",
    "    num_classes = 43\n",
    "    \n",
    "    X_extended = np.empty([0, X.shape[1], X.shape[2], X.shape[3]], dtype = X.dtype)\n",
    "    y_extended = np.empty([0], dtype = y.dtype)\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        # First copy existing data for this class\n",
    "        X_extended = np.append(X_extended, X[y == c], axis = 0)\n",
    "        # If we can flip images of this class horizontally and they would still belong to said class...\n",
    "        if c in self_flippable:\n",
    "            # ...Copy their flipped versions into extended array.\n",
    "            X_extended = np.append(X_extended, X[y == c][:, :, ::-1, :], axis = 0)\n",
    "        # If we can flip images of this class horizontally and they would belong to other class...\n",
    "        if c in cross_flippable[:, 0]:\n",
    "            # ...Copy flipped images of that other class to the extended array.\n",
    "            flip_class = cross_flippable[cross_flippable[:, 0] == c][0][1]\n",
    "            X_extended = np.append(X_extended, X[y == flip_class][:, :, ::-1, :], axis = 0)\n",
    "        # Fill labels for added images set to current class.\n",
    "        y_extended = np.append(y_extended, np.full((X_extended.shape[0] - y_extended.shape[0]), c, dtype = int))\n",
    "    \n",
    "    return (X_extended, y_extended)\n",
    "\n",
    "def extend_balancing_classes(X, y):\n",
    "    \"\"\"\n",
    "    Extends dataset by duplicating existing images while applying data augmentation pipeline.\n",
    "    Number of examples for each class is extended to match the most represented class, thus\n",
    "    balancing the dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X       : ndarray\n",
    "              Dataset array containing feature examples.\n",
    "    y       : ndarray, optional, defaults to `None`\n",
    "              Dataset labels in index form.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of X and y.    \n",
    "    \"\"\"\n",
    "    num_classes = 43\n",
    "\n",
    "    _, class_counts = np.unique(y, return_counts = True)\n",
    "    max_c = max(class_counts)\n",
    "    \n",
    "    X_extended = np.empty([0, X.shape[1], X.shape[2], X.shape[3]], dtype = np.float32)\n",
    "    y_extended = np.empty([0], dtype = y.dtype)\n",
    "    \n",
    "    for c, c_count in zip(range(num_classes), class_counts):\n",
    "        # First copy existing data for this class\n",
    "        X_source = (X[y == c] / 255.).astype(np.float32)\n",
    "        y_source = y[y == c]\n",
    "        print(\"Extending data for class {}: augmenting {} examples to match {}...\".format(c, c_count, max_c))\n",
    "        print(\"    Appending {} examples from original dataset.\".format(X_source.shape[0]))\n",
    "        X_extended = np.append(X_extended, X_source, axis = 0)\n",
    "        print(\"    Appending {} batches of {} augmented examples.\".format((max_c // c_count) - 1, X_source.shape[0]))\n",
    "        for i in range((max_c // c_count) - 1):\n",
    "            batch_iterator = AugmentedSignsBatchIterator(batch_size = X_source.shape[0], p = 1.0)\n",
    "            for x_batch, _ in batch_iterator(X_source, y_source):\n",
    "                X_extended = np.append(X_extended, x_batch, axis = 0)\n",
    "        batch_iterator = AugmentedSignsBatchIterator(batch_size = max_c % c_count, p = 1.0)\n",
    "        for x_batch, _ in batch_iterator(X_source, y_source):\n",
    "            print(\"    Appending final batch of remaining {} augmented examples.\".format(x_batch.shape[0]))\n",
    "            X_extended = np.append(X_extended, x_batch, axis = 0)\n",
    "            break\n",
    "        # Fill labels for added images set to current class.\n",
    "        added = X_extended.shape[0] - y_extended.shape[0]\n",
    "        y_extended = np.append(y_extended, np.full((added), c, dtype = int))\n",
    "        print(\"...{} examples added, {} total.\".format(added - c_count, added))\n",
    "        \n",
    "    return ((X_extended * 255.).astype(np.uint8), y_extended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we preprocess the whole dataset first. We first flip some of the images which classes allow that, and then extend examples for each class so that the dataset is balanced. We don't simply copy the images — we apply augmentation, so that every time we duplicate the data it's slightly different.\n",
    "\n",
    "We presumably only need to do this once, then pickle the resulting extended training dataset and use that pickle from that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "\n",
    "X_train, y_train = load_pickled_data(train_dataset_file, ['features', 'labels'])\n",
    "print(\"Number of training examples in initial dataset =\", X_train.shape[0])\n",
    "X_train, y_train = extend_flipping_horizontally(X_train, y_train)\n",
    "print(\"Number of training examples after horizontal flipping =\", X_train.shape[0])\n",
    "X_train, y_train = extend_balancing_classes(X_train, y_train)\n",
    "print(\"Number of training examples after balancing classes =\", X_train.shape[0])\n",
    "\n",
    "# pickle.dump({\n",
    "#         \"features\" : X_train,\n",
    "#         \"labels\" : y_train\n",
    "#     }, open(train_balanced_dataset_file, \"wb\" ) )\n",
    "\n",
    "# print(\"Balanced dataset saved in \", train_balanced_dataset_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "\n",
    "_Describe the techniques used to preprocess the data._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "I convert images to grayscale, scale to `[0, 1]` and shuffle the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "_Describe how you set up the training, validation and testing data for your model. If you generated additional data, why?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "I decided to use a 20% slice of training data for validation. Normally I would split entire dataset as 70% training, 15% validation, 15% testing, but since here test dataset was supplied separately I assumed it's better to use all of it solely for testing.\n",
    "\n",
    "The dataset looked quite small and some classes were significantly less represented than others, so I reckon the model wouldn't generalise well without additional examples. I decided to use data augmentation by applying randomised transformations while supplying batches of training data to the model. I find Lasagne's `BatchIterator` class very convenient, as one can simply override the `transform()` function and provide their data augmentation pipeline there.\n",
    "\n",
    "So that's what I did, my pipeline includes:\n",
    "* **Rotation**. Rotates the image at a random angle in the range of `[-15, 15]` degrees.\n",
    "* **Projection transform**. Warps the image by projecting each of the image corners to a random point within a 10% of image size distance from the original point.\n",
    "* **Gamma adjustment**. Adjusts image brightness/contrast by applying gamma adjustment in the range of `[0.5, 1.5]`.\n",
    "* **Blur**. Applies gaussian blur with sigma in the range of `[0, 0.8]`.\n",
    "* **Noise**. Adds random noise to the image. I've switched it off for now, but might experiment with it later.\n",
    "* Last but not least, **horizontal flip**. Flips images horizontally if their class would remain the same, *OR* flips and updates the label, if flipped image represents some other class (e.g. flipped *Turn left* sign would become *Turn right*).\n",
    "\n",
    "I chose these transformations as they seemed to be close to what we could see on the photos. Image could be blurred and/or noisy due to low resolution source photo, it can be either too bright or too dark depending on the outside conditions, and it can look rotated and/or projection warped on the photo depending on the shooting angle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Struct to organise some of the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Parameters = namedtuple('Parameters', [\n",
    "        # Data parameters\n",
    "        'num_classes', 'image_size', 'valid_size',\n",
    "        # Training parameters\n",
    "        'batch_size', 'max_epochs', 'log_epoch', \n",
    "        # Optimisations\n",
    "        'data_augmentation', 'data_augmentation_p', 'data_augmentation_intensity', \n",
    "        'learning_rate_decay', 'learning_rate', 'momentum_increase', 'dropout_enabled', \n",
    "        'l2_reg_enabled', 'l2_lambda', 'stage2', 'early_stopping_enabled', \n",
    "        'early_stopping_patience', 'resume_training', \n",
    "        # Layers architecture\n",
    "        'conv1_k', 'conv1_d', 'conv1_p', \n",
    "        'conv2_k', 'conv2_d', 'conv2_p', \n",
    "        'conv3_k', 'conv3_d', 'conv3_p', \n",
    "        'fc4_size', 'fc4_p', \n",
    "        'fc5_size', 'fc5_p'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "def get_time_hhmmss(start):\n",
    "    \"\"\"\n",
    "    Calculates time since `start` and formats as a string.\n",
    "    \"\"\"\n",
    "    end = time.time()\n",
    "    m, s = divmod(end - start, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    time_str = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "    return time_str\n",
    "\n",
    "class Paths(object):\n",
    "    \"\"\"\n",
    "    Provides easy access to common paths we use for persisting \n",
    "    the data associated with model training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        \"\"\"\n",
    "        Initialises a new `Paths` instance and creates corresponding folders if needed.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params  : Parameters\n",
    "                  Structure (`namedtuple`) containing model parameters.\n",
    "        \"\"\"\n",
    "        self.model_name = self.get_model_name(params)\n",
    "        self.var_scope = self.get_variables_scope(params)\n",
    "        self.root_path = os.getcwd() + \"/models/\" + self.model_name + \"/\"\n",
    "        self.model_path = self.get_model_path()\n",
    "        self.train_history_path = self.get_train_history_path()\n",
    "        os.makedirs(self.root_path, exist_ok = True)\n",
    "\n",
    "    def get_model_name(self, params):\n",
    "        \"\"\"\n",
    "        Generates a model name with some of the crucial model parameters encoded into the name.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params  : Parameters\n",
    "                  Structure (`namedtuple`) containing model parameters.\n",
    "                  \n",
    "        Returns\n",
    "        -------\n",
    "        Model name.\n",
    "        \"\"\"\n",
    "        # We will encode model settings in its name: architecture, optimisations applied, etc.\n",
    "        model_name = \"k{}d{}p{}_k{}d{}p{}_k{}d{}p{}_fc{}p{}_fc{}p{}\".format(\n",
    "            params.conv1_k, params.conv1_d, params.conv1_p, \n",
    "            params.conv2_k, params.conv2_d, params.conv2_p, \n",
    "            params.conv3_k, params.conv3_d, params.conv3_p, \n",
    "            params.fc4_size, params.fc4_p, \n",
    "            params.fc5_size, params.fc5_p\n",
    "        )\n",
    "        model_name += \"_aug-p{}i{}\".format(params.data_augmentation_p, params.data_augmentation_intensity) if params.data_augmentation else \"_no-aug\"\n",
    "        model_name += \"_st2\" if params.stage2 else \"_no-st2\"\n",
    "#         model_name += \"_lrdec\" if params.learning_rate_decay else \"_no-lrdec\"\n",
    "#         model_name += \"_mominc\" if params.momentum_increase else \"_no-mom\"\n",
    "#         model_name += \"_dr\" if params.dropout_enabled else \"_no-dr\"\n",
    "#         model_name += \"_l2\" if params.l2_reg_enabled else \"_no-l2\"\n",
    "        return model_name\n",
    "\n",
    "    def get_variables_scope(self, params):\n",
    "        \"\"\"\n",
    "        Generates a model variable scope with some of the crucial model parameters encoded.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params  : Parameters\n",
    "                  Structure (`namedtuple`) containing model parameters.\n",
    "                  \n",
    "        Returns\n",
    "        -------\n",
    "        Variables scope name.\n",
    "        \"\"\"\n",
    "        # We will encode model settings in its name: architecture, optimisations applied, etc.\n",
    "        var_scope = \"k{}d{}_k{}d{}_k{}d{}_fc{}_fc{}\".format(\n",
    "            params.conv1_k, params.conv1_d,\n",
    "            params.conv2_k, params.conv2_d,\n",
    "            params.conv3_k, params.conv3_d, \n",
    "            params.fc4_size,\n",
    "            params.fc5_size,\n",
    "        )\n",
    "        var_scope += \"_st2\" if params.stage2 else \"_no-st2\"\n",
    "#         var_scope += \"_l2\" if params.l2_reg_enabled else \"_no-l2\"\n",
    "        return var_scope\n",
    "\n",
    "    def get_model_path(self):\n",
    "        \"\"\"\n",
    "        Generates path to the model file.\n",
    "   \n",
    "        Returns\n",
    "        -------\n",
    "        Model file path.\n",
    "        \"\"\"\n",
    "        return self.root_path + \"model.ckpt\"\n",
    "\n",
    "    def get_train_history_path(self):\n",
    "        \"\"\"\n",
    "        Generates path to the train history file.\n",
    "   \n",
    "        Returns\n",
    "        -------\n",
    "        Train history file path.\n",
    "        \"\"\"\n",
    "        return self.root_path + \"train_history\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    \"\"\"\n",
    "    Provides early stopping functionality. Keeps track of model accuracy, \n",
    "    and if it doesn't improve over time restores last best performing \n",
    "    parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, saver, session, patience = 100):\n",
    "        \"\"\"\n",
    "        Initialises a `EarlyStopping` isntance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        saver     : \n",
    "                    TensorFlow Saver object to be used for saving and restoring model.\n",
    "        session   : \n",
    "                    TensorFlow Session object containing graph where model is restored.\n",
    "        patience  : \n",
    "                    Early stopping patience. This is the number of epochs we wait for \n",
    "                    accuracy to start improving again before stopping and restoring \n",
    "                    previous best performing parameters.\n",
    "                  \n",
    "        Returns\n",
    "        -------\n",
    "        New instance.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.saver = saver\n",
    "        self.session = session\n",
    "        self.best_accuracy = 0.\n",
    "        self.best_accuracy_epoch = 0\n",
    "        self.restore_path = None\n",
    "\n",
    "    def __call__(self, accuracy, epoch):\n",
    "        \"\"\"\n",
    "        Checks if we need to stop and restores the last well performing values if we do.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        accuracy  : \n",
    "                    Last epoch accuracy.\n",
    "        epoch     : \n",
    "                    Last epoch number.\n",
    "                  \n",
    "        Returns\n",
    "        -------\n",
    "        `True` if we waited enough and it's time to stop and we restored the \n",
    "        best performing weights, or `False` otherwise.\n",
    "        \"\"\"\n",
    "        if accuracy > self.best_accuracy:\n",
    "            self.best_accuracy = accuracy\n",
    "            self.best_accuracy_epoch = epoch\n",
    "            self.restore_path = self.saver.save(self.session, os.getcwd() + \"/early_stopping_checkpoint\")\n",
    "        elif self.best_accuracy_epoch + self.patience < epoch:\n",
    "            print(\"Early stopping.\\nBest monitored accuracy was {:.3f}%% at epoch {}.\".format(\n",
    "                self.best_accuracy, self.best_accuracy_epoch\n",
    "            ))\n",
    "            if self.restore_path != None:\n",
    "                self.saver.restore(self.session, self.restore_path)\n",
    "            else:\n",
    "                print(\"ERROR: Failed to restore session\")\n",
    "            return True\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source class: No passing\n",
      " Batch class: No passing\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'image_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-78e4acdef841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_size' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAH0AAABhCAYAAAD7lK7FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAAZxJREFUeJzt1rGNwkAQRuF/TwTOaYDUKU3Qg+XyaIJKnNIAOdlccASX\n2FhCgOC9T5psVlrpyfK2qopYft59Ab2e0YGMDmR0IKMDGR3I6EBGB9qsWWqtbZMckpyTXJ95IT2k\nS7JLcqqqy+xWVd2dJEOScj5mhqWeq770/H3hOR6P6ft+5RG92jRNGccxufWaszb6NUn6vs9+v3/s\nZnqFxV+wDzkgowMZHcjoQEYHMjqQ0YGMDmR0IKMDGR3I6EBGBzI6kNGBjA5kdCCjAxkdyOhARgcy\nOpDRgYwOZHQgowMZHcjoQEYHMjqQ0YGMDmR0IKMDGR3I6EBGBzI6kNGBjA5kdCCjAxkdyOhARgcy\nOpDRgYwOZHQgowMZHcjoQEYHMjqQ0YGMDmR0IKMDGR3I6EBGBzI6kNGBjA5kdCCjAxkdyOhARgcy\nOpDRgYwOZHSgzcq9LkmmaXriVfSof326xcWqujtJhiTlfMwMSz3bLeqi1to2ySHJOcn17gG9S5dk\nl+RUVZe5pVXR9V18yAEZHcjoQEYHMjqQ0YGMDvQLUwtd14ZcyNwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f00321763c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_iterator = AugmentedSignsBatchIterator(batch_size = 20)\n",
    "for x_batch, y_batch in batch_iterator(X_train, y_train):\n",
    "    for i in range(20): \n",
    "        print(\"Source class: \" + class_name(y_train[i]) + \"\\n Batch class: \" + class_name(y_batch[i]))\n",
    "        # plot two images:\n",
    "        fig = pyplot.figure(figsize=(3, 1))\n",
    "        axis = fig.add_subplot(1, 2, 1, xticks=[], yticks=[])\n",
    "        axis.imshow(X_train[i].reshape(image_size, image_size), cmap='gray')\n",
    "        axis = fig.add_subplot(1, 2, 2, xticks=[], yticks=[])\n",
    "        axis.imshow(x_batch[i].reshape(image_size, image_size), cmap='gray')\n",
    "        pyplot.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logger to keep track of the training even if no browser is connected to kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    \"\"\"\n",
    "    Duplicates console logs into a text file.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_path):\n",
    "        self.path = root_path + \"log.txt\"\n",
    "        os.makedirs(root_path, exist_ok = True)\n",
    "\n",
    "    def __call__(self, string):\n",
    "        \"\"\"\n",
    "        Logs the value to console and appends the same string to the log file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        string  : string\n",
    "                  Value to be logged.\n",
    "        \"\"\"\n",
    "        print(string)\n",
    "        with open(self.path, 'a') as file:\n",
    "            file.write(string + '\\n')\n",
    "            \n",
    "    def clear(self):\n",
    "        \"\"\"\n",
    "        Removes log file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            os.remove(self.path)\n",
    "        except OSError:\n",
    "            pass    \n",
    "\n",
    "    def log_parameters(self, params, train_size, valid_size, test_size):\n",
    "        \"\"\"\n",
    "        Logs model parameters to console and appends the same text representation to the log file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params    : Parameters\n",
    "                    Structure (`namedtuple`) containing model parameters.\n",
    "        train_size: int\n",
    "                    Size of the training dataset.\n",
    "        valid_size: int\n",
    "                    Size of the training dataset.\n",
    "        test_size : int\n",
    "                    Size of the training dataset.\n",
    "        \"\"\"\n",
    "        if params.resume_training:\n",
    "            self(\"=================================================\")\n",
    "            self(\"=============== RESUMING TRAINING ===============\")\n",
    "            self(\"=================================================\")\n",
    "\n",
    "        self(\"===================== DATA ======================\")\n",
    "        self(\"            Training set: {} examples\".format(train_size))\n",
    "        self(\"          Validation set: {} examples\".format(valid_size))\n",
    "        self(\"             Testing set: {} examples\".format(test_size))\n",
    "        self(\"              Batch size: {}\".format(params.batch_size))   \n",
    "\n",
    "        self(\"===================== MODEL =====================\")\n",
    "        self(\"----------------- ARCHITECTURE ------------------\")  \n",
    "        self(\" %-*s %-*s %-*s %-*s\" % (10, \"\", 10, \"Type\", 8, \"Size\", 15, \"Dropout (keep p)\"))    \n",
    "        self(\" %-*s %-*s %-*s %-*s\" % (10, \"Layer 1\", 10, \"{}x{} Conv\".format(params.conv1_k, params.conv1_k), 8, str(params.conv1_d), 15, str(params.conv1_p)))    \n",
    "        self(\" %-*s %-*s %-*s %-*s\" % (10, \"Layer 2\", 10, \"{}x{} Conv\".format(params.conv2_k, params.conv2_k), 8, str(params.conv2_d), 15, str(params.conv2_p)))    \n",
    "        self(\" %-*s %-*s %-*s %-*s\" % (10, \"Layer 3\", 10, \"{}x{} Conv\".format(params.conv3_k, params.conv3_k), 8, str(params.conv3_d), 15, str(params.conv3_p)))    \n",
    "        self(\" %-*s %-*s %-*s %-*s\" % (10, \"Layer 4\", 10, \"FC\", 8, str(params.fc4_size), 15, str(params.fc4_p)))    \n",
    "        self(\" %-*s %-*s %-*s %-*s\" % (10, \"Layer 5\", 10, \"FC\", 8, str(params.fc5_size), 15, str(params.fc5_p))) \n",
    "        self(\"------------------ PARAMETERS -------------------\")\n",
    "        self(\"       Data augmentation: \" + (\"Enabled (on {}% of data with intensity = {})\".format(params.data_augmentation_p * 100, params.data_augmentation_intensity) if params.data_augmentation else \"Disabled\"))\n",
    "        self(\"     Learning rate decay: \" + (\"Enabled\" if params.learning_rate_decay else \"Disabled (rate = {})\".format(params.learning_rate)))\n",
    "        self(\"       Momentum increase: \" + (\"Enabled\" if params.momentum_increase else \"Disabled\"))\n",
    "        self(\"                 Dropout: \" + (\"Enabled\" if params.dropout_enabled else \"Disabled\"))\n",
    "        self(\"       L2 Regularization: \" + (\"Enabled (lambda = {})\".format(params.l2_lambda) if params.l2_reg_enabled else \"Disabled\"))\n",
    "        self(\"      2nd Stage training: \" + (\"Enabled\" if params.stage2 else \"Disabled\"))\n",
    "        self(\"          Early stopping: \" + (\"Enabled (patience = {})\".format(params.early_stopping_patience) if params.early_stopping_enabled else \"Disabled\"))\n",
    "        self(\" Keep training old model: \" + (\"Enabled\" if params.resume_training else \"Disabled\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(input, size):\n",
    "    \"\"\"\n",
    "    Performs a single fully connected layer pass, e.g. returns `input * weights + bias`.\n",
    "    \"\"\"\n",
    "    weights = tf.get_variable( 'weights', \n",
    "        shape = [input.get_shape()[1], size],\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "      )\n",
    "    biases = tf.get_variable( 'biases',\n",
    "        shape = [size],\n",
    "        initializer = tf.constant_initializer(0.0)\n",
    "      )\n",
    "    return tf.matmul(input, weights) + biases\n",
    "\n",
    "def conv_relu(input, kernel_size, depth):\n",
    "    \"\"\"\n",
    "    Performs a single convolution layer pass.\n",
    "    \"\"\"\n",
    "    weights = tf.get_variable( 'weights', \n",
    "        shape = [kernel_size, kernel_size, input.get_shape()[3], depth],\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "      )\n",
    "    biases = tf.get_variable( 'biases',\n",
    "        shape = [depth],\n",
    "        initializer = tf.constant_initializer(0.0)\n",
    "      )\n",
    "    conv = tf.nn.conv2d(input, weights,\n",
    "        strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "def pool(input, size):\n",
    "    \"\"\"\n",
    "    Performs a max pooling layer pass.\n",
    "    \"\"\"\n",
    "    return tf.nn.max_pool(\n",
    "        input, \n",
    "        ksize = [1, size, size, 1], \n",
    "        strides = [1, size, size, 1], \n",
    "        padding = 'SAME'\n",
    "    )\n",
    "\n",
    "def model_pass(input, params):\n",
    "    \"\"\"\n",
    "    Performs a full model pass.\n",
    "    \"\"\"\n",
    "    # Convolutions\n",
    "    \n",
    "    with tf.variable_scope('conv1'):\n",
    "        conv1 = conv_relu(input, kernel_size = params.conv1_k, depth = params.conv1_d) \n",
    "    with tf.variable_scope('pool1'): \n",
    "        pool1 = pool(conv1, size = 2)\n",
    "        if params.dropout_enabled: pool1 = tf.nn.dropout(pool1, keep_prob = params.conv1_p)\n",
    "    with tf.variable_scope('conv2'):\n",
    "        conv2 = conv_relu(pool1, kernel_size = params.conv2_k, depth = params.conv2_d)\n",
    "    with tf.variable_scope('pool2'):\n",
    "        pool2 = pool(conv2, size = 2)\n",
    "        if params.dropout_enabled: pool2 = tf.nn.dropout(pool2, keep_prob = params.conv2_p)\n",
    "    with tf.variable_scope('conv3'):\n",
    "        conv3 = conv_relu(pool2, kernel_size = params.conv3_k, depth = params.conv3_d)\n",
    "    with tf.variable_scope('pool3'):\n",
    "        pool3 = pool(conv3, size = 2)\n",
    "        if params.dropout_enabled: pool3 = tf.nn.dropout(pool3, keep_prob = params.conv3_p)\n",
    "    \n",
    "    # Fully connected\n",
    "    \n",
    "    # 1st stage output\n",
    "    shape = pool3.get_shape().as_list()\n",
    "    pool3 = tf.reshape(pool3, [-1, shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    if params.stage2:\n",
    "        # 2nd stage output\n",
    "        shape = pool1.get_shape().as_list()\n",
    "        pool1 = tf.reshape(pool1, [-1, shape[1] * shape[2] * shape[3]])\n",
    "        flattened = tf.concat(1, [pool1, pool3])\n",
    "    else:\n",
    "        flattened = pool3\n",
    "    \n",
    "    with tf.variable_scope('fc4'):\n",
    "        fc4 = fully_connected(flattened, size = params.fc4_size)\n",
    "        if params.dropout_enabled: fc4 = tf.nn.dropout(fc4, keep_prob = params.fc4_p)\n",
    "    with tf.variable_scope('fc5'):\n",
    "        fc5 = fully_connected(fc4, size = params.fc5_size)\n",
    "        if params.dropout_enabled: fc4 = tf.nn.dropout(fc4, keep_prob = params.fc5_p)\n",
    "    with tf.variable_scope('out'):\n",
    "        prediction = fully_connected(fc5, size = params.num_classes)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "_What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.)  For reference on how to build a deep neural network using TensorFlow, see [Deep Neural Network in TensorFlow\n",
    "](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/b516a270-8600-4f93-a0a3-20dfeabe5da6/concepts/83a3a2a2-a9bd-4b7b-95b0-eb924ab14432) from the classroom._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(params, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Performs model training based on provided training dataset \n",
    "    according to provided parameters, and then evaluates trained \n",
    "    model with testing dataset. \n",
    "    Part of the training dataset may be used for validation during\n",
    "    training if specified in model parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params    : Parameters\n",
    "                Structure (`namedtuple`) containing model parameters.\n",
    "    X_train   : \n",
    "                Training dataset. \n",
    "    y_train   : \n",
    "                Training dataset labels. \n",
    "    X_test    : \n",
    "                Testing dataset. \n",
    "    y_test    : \n",
    "                Testing dataset labels. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialisation routines: generate variable scope, create logger, note start time.\n",
    "    paths = Paths(params)\n",
    "    log = Logger(paths.root_path)\n",
    "    start = time.time()\n",
    "    model_variable_scope = paths.var_scope\n",
    "\n",
    "    # Prepare data: use portion of training data for validation\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = params.valid_size)\n",
    "    \n",
    "    if not params.resume_training: log.clear()\n",
    "    log.log_parameters(params, y_train.shape[0], y_valid.shape[0], y_test.shape[0]) \n",
    "    \n",
    "    # Build the graph\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Input data. For the training data, we use a placeholder that will be fed at run time with a training minibatch.\n",
    "        tf_x_batch = tf.placeholder(tf.float32, shape = (None, params.image_size[0], params.image_size[1], 1))\n",
    "        tf_y_batch = tf.placeholder(tf.float32, shape = (None, params.num_classes))\n",
    "        current_epoch = tf.Variable(0)  # count the number of epochs\n",
    "\n",
    "        # Model parameters.\n",
    "        if params.learning_rate_decay:\n",
    "            learning_rate = tf.train.exponential_decay(0.03, current_epoch, decay_steps = params.max_epochs, decay_rate = 0.03)\n",
    "        else:\n",
    "            learning_rate = params.learning_rate\n",
    "            \n",
    "        momentum = 0.9 + (0.99 - 0.9) * (current_epoch / params.max_epochs) if params.momentum_increase else 0.99\n",
    "\n",
    "        # Training computation.\n",
    "        with tf.variable_scope(model_variable_scope):\n",
    "            logits = model_pass(tf_x_batch, params)\n",
    "            if params.l2_reg_enabled:\n",
    "                with tf.variable_scope('fc4', reuse = True):\n",
    "                    l2_loss = tf.nn.l2_loss(tf.get_variable('weights')) \n",
    "                with tf.variable_scope('fc5', reuse = True):\n",
    "                    l2_loss += tf.nn.l2_loss(tf.get_variable('weights'))\n",
    "            else:\n",
    "                l2_loss = 0\n",
    "\n",
    "        predictions = tf.nn.softmax(logits)\n",
    "        softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, tf_y_batch)\n",
    "        loss = tf.reduce_mean(softmax_cross_entropy) + params.l2_lambda * l2_loss  \n",
    "\n",
    "        # Optimizer.\n",
    "        optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate = learning_rate\n",
    "        ).minimize(loss)\n",
    "\n",
    "    with tf.Session(graph = graph) as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        # A routine for evaluating current model parameters\n",
    "        def get_accuracy_and_loss_in_batches(X, y):\n",
    "            p = []\n",
    "            sce = []\n",
    "            batch_iterator = BatchIterator(batch_size = 128)\n",
    "            for x_batch, y_batch in batch_iterator(X, y):\n",
    "                [p_batch, sce_batch] = session.run([predictions, softmax_cross_entropy], feed_dict = {\n",
    "                        tf_x_batch : x_batch, \n",
    "                        tf_y_batch : y_batch\n",
    "                    }\n",
    "                )\n",
    "                p.extend(p_batch)\n",
    "                sce.extend(sce_batch)\n",
    "            p = np.array(p)\n",
    "            sce = np.array(sce)\n",
    "            accuracy = 100.0 * np.sum(np.argmax(p, 1) == np.argmax(y, 1)) / p.shape[0]\n",
    "            loss = np.mean(sce)\n",
    "            return (accuracy, loss)\n",
    "        \n",
    "        # If we chose to keep training previously trained model, restore session.\n",
    "        if params.resume_training: tf.train.Saver().restore(session, paths.model_path)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        early_stopping = EarlyStopping(tf.train.Saver(), session, patience = params.early_stopping_patience)\n",
    "        train_loss_history = np.zeros(params.max_epochs)\n",
    "        train_accuracy_history = np.zeros(params.max_epochs)\n",
    "        valid_loss_history = np.zeros(params.max_epochs)\n",
    "        valid_accuracy_history = np.zeros(params.max_epochs)\n",
    "        log(\"=================== TRAINING ====================\")\n",
    "        for epoch in range(params.max_epochs):\n",
    "            current_epoch = epoch\n",
    "            # Train on whole randomised dataset in batches\n",
    "            if params.data_augmentation:\n",
    "                batch_iterator = AugmentedSignsBatchIterator(batch_size = params.batch_size, shuffle = True, p = params.data_augmentation_p, intensity = params.data_augmentation_intensity)  \n",
    "            else: \n",
    "                batch_iterator = BatchIterator(batch_size = params.batch_size, shuffle = True)\n",
    "            for x_batch, y_batch in batch_iterator(X_train, y_train):\n",
    "                session.run([optimizer], feed_dict = {\n",
    "                        tf_x_batch : x_batch, \n",
    "                        tf_y_batch : y_batch\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # If another significant epoch ended, we log our losses.\n",
    "            if (epoch % params.log_epoch == 0):\n",
    "                # Get validation data predictions and log validation loss:\n",
    "                valid_accuracy, valid_loss = get_accuracy_and_loss_in_batches(X_valid, y_valid)\n",
    "                valid_loss_history[epoch] = valid_loss\n",
    "                valid_accuracy_history[epoch] = valid_accuracy\n",
    "\n",
    "                # Get training data predictions and log training loss:\n",
    "                train_accuracy, train_loss = get_accuracy_and_loss_in_batches(X_train, y_train)\n",
    "                train_loss_history[epoch] = train_loss\n",
    "                train_accuracy_history[epoch] = train_accuracy\n",
    "\n",
    "                if (epoch % 50 == 0):\n",
    "                    log(\"---------------- EPOCH %4d/%d ----------------\" % (epoch + 1, params.max_epochs))\n",
    "                    log(\"     Train loss: %.8f, accuracy: %.2f%%\" % (train_loss, train_accuracy))\n",
    "                    log(\"Validation loss: %.8f, accuracy: %.2f%%\" % (valid_loss, valid_accuracy))\n",
    "                    log(\"           Time: \" + get_time_hhmmss(start))\n",
    "\n",
    "            if params.early_stopping_enabled:\n",
    "                # Get validation data predictions and log validation loss:\n",
    "                valid_accuracy, _ = get_accuracy_and_loss_in_batches(X_valid, y_valid)\n",
    "                if early_stopping(valid_accuracy, epoch): break\n",
    "\n",
    "        # Evaluate on test dataset.\n",
    "        test_accuracy, test_loss = get_accuracy_and_loss_in_batches(X_test, y_test)\n",
    "        log(\"=================================================\")\n",
    "        log(\" Test loss: %.8f, accuracy = %.2f%%)\" % (test_loss, test_accuracy)) \n",
    "        log(\" Total time: \" + get_time_hhmmss(start))\n",
    "\n",
    "        # Save model weights for future use.\n",
    "        saved_model_path = saver.save(session, paths.model_path)\n",
    "        log(\"Model file: \" + saved_model_path)\n",
    "        np.savez(paths.train_history_path, train_loss_history = train_loss_history, train_accuracy_history = train_accuracy_history, valid_loss_history = valid_loss_history, valid_accuracy_history = valid_accuracy_history)\n",
    "        log(\"Train history file: \" + paths.train_history_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================\n",
      "=============== RESUMING TRAINING ===============\n",
      "=================================================\n",
      "===================== DATA ======================\n",
      "            Training set: 148608 examples\n",
      "          Validation set: 37152 examples\n",
      "             Testing set: 12630 examples\n",
      "              Batch size: 128\n",
      "===================== MODEL =====================\n",
      "----------------- ARCHITECTURE ------------------\n",
      "            Type       Size     Dropout (keep p)\n",
      " Layer 1    5x5 Conv   32       0.8            \n",
      " Layer 2    5x5 Conv   64       0.7            \n",
      " Layer 3    5x5 Conv   128      0.6            \n",
      " Layer 4    FC         1024     0.5            \n",
      " Layer 5    FC         1024     0.8            \n",
      "------------------ PARAMETERS -------------------\n",
      "       Data augmentation: Enabled (on 50.0% of data with intensity = 0.75)\n",
      "     Learning rate decay: Disabled (rate = 0.001)\n",
      "       Momentum increase: Disabled\n",
      "                 Dropout: Enabled\n",
      "       L2 Regularization: Enabled (lambda = 0.0001)\n",
      "      2nd Stage training: Enabled\n",
      "          Early stopping: Disabled\n",
      " Keep training old model: Enabled\n",
      "=================== TRAINING ====================\n",
      "---------------- EPOCH    1/500 ----------------\n",
      "     Train loss: 0.19587341, accuracy: 95.10%\n",
      "Validation loss: 0.19480473, accuracy: 94.93%\n",
      "           Time: 00:01:31\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "parameters = Parameters(\n",
    "    # Data parameters\n",
    "    num_classes = 43,\n",
    "    image_size = (32, 32),\n",
    "    valid_size = 0.2,\n",
    "    # Training parameters\n",
    "    batch_size = 128,\n",
    "    max_epochs = 500,\n",
    "    log_epoch = 5,\n",
    "    # Optimisations\n",
    "    data_augmentation = True,\n",
    "    data_augmentation_p = 0.5,\n",
    "    data_augmentation_intensity = 0.75,\n",
    "    learning_rate_decay = False,\n",
    "    learning_rate = 0.001,\n",
    "    momentum_increase = False,\n",
    "    dropout_enabled = True,\n",
    "    l2_reg_enabled = True,\n",
    "    l2_lambda = 0.0001,\n",
    "    stage2 = True,\n",
    "    early_stopping_enabled = False,\n",
    "    early_stopping_patience = 100,\n",
    "    resume_training = True,\n",
    "    # Layers architecture\n",
    "    conv1_k = 5, conv1_d = 32, conv1_p = 0.8,\n",
    "    conv2_k = 5, conv2_d = 64, conv2_p = 0.7,\n",
    "    conv3_k = 5, conv3_d = 128, conv3_p = 0.6,\n",
    "    fc4_size = 1024, fc4_p = 0.5,\n",
    "    fc5_size = 1024, fc5_p = 0.8\n",
    ")\n",
    "\n",
    "# Load and preprocess initial data by scaling features, shuffling the data, balancing classes, etc.\n",
    "X_train, y_train = load_and_process_data(\"traffic-signs-data/train_balanced.p\")\n",
    "X_test, y_test = load_and_process_data(\"traffic-signs-data/test.p\")\n",
    "\n",
    "train_model(parameters, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA10AAAF8CAYAAAA0FNHlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAIW5PTrNAAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f006e88fe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plots history of learning curves for a specific model. You may want to call `pyplot.show()` afterwards.\n",
    "def plot_learning_curves(axis, params, train_column, valid_column, linewidth = 2, train_linestyle = \"b-\", valid_linestyle = \"g-\"):\n",
    "    model_history = np.load(Paths(params).train_history_path + \".npz\")\n",
    "    train_values = model_history[train_column]\n",
    "    valid_values = model_history[valid_column]\n",
    "    epochs = train_values.shape[0]\n",
    "    x_axis = np.arange(epochs)\n",
    "    axis.plot(x_axis[train_values > 0], train_values[train_values > 0], train_linestyle, linewidth=linewidth, label=\"train\")\n",
    "    axis.plot(x_axis[valid_values > 0], valid_values[valid_values > 0], valid_linestyle, linewidth=linewidth, label=\"valid\")\n",
    "    return train_values[train_values > 0].shape[0]\n",
    "\n",
    "curves_figure = pyplot.figure(figsize = (10, 4))\n",
    "axis = curves_figure.add_subplot(1, 2, 1)\n",
    "\n",
    "new_model_epochs = plot_learning_curves(axis, parameters, train_column = \"train_accuracy_history\", valid_column = \"valid_accuracy_history\")\n",
    "old_model_epochs = 0 #plot_learning_curves(\"3con_2fc_b36_e1000_aug_lrdec_mominc\", linewidth = 1)\n",
    "\n",
    "pyplot.grid()\n",
    "pyplot.legend()\n",
    "pyplot.xlabel(\"epoch\")\n",
    "pyplot.ylabel(\"accuracy\")\n",
    "pyplot.ylim(50., 102.)\n",
    "pyplot.xlim(0, max(new_model_epochs, old_model_epochs))\n",
    "\n",
    "axis = curves_figure.add_subplot(1, 2, 2)\n",
    "new_model_epochs = plot_learning_curves(axis, parameters, train_column = \"train_loss_history\", valid_column = \"valid_loss_history\")\n",
    "old_model_epochs = 0 #plot_learning_curves(\"3con_2fc_b36_e1000_aug_lrdec_mominc\", linewidth = 1)\n",
    "\n",
    "pyplot.grid()\n",
    "pyplot.legend()\n",
    "pyplot.xlabel(\"epoch\")\n",
    "pyplot.ylabel(\"loss\")\n",
    "pyplot.ylim(0.0001, 10.)\n",
    "pyplot.xlim(0, max(new_model_epochs, old_model_epochs))\n",
    "pyplot.yscale(\"log\")\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "_How did you train your model? (Type of optimizer, batch size, epochs, hyperparameters, etc.)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "\n",
    "_What approach did you take in coming up with a solution to this problem?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "Take several pictures of traffic signs that you find on the web or around you (at least five), and run them through your classifier on your computer to produce example results. The classifier might not recognize some local signs but it could prove interesting nonetheless.\n",
    "\n",
    "You may find `signnames.csv` useful as it contains mappings from the class id (integer) to the actual sign name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load the images and plot them here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "_Choose five candidate images of traffic signs and provide them in the report. Are there any particular qualities of the image(s) that might make classification difficult? It would be helpful to plot the images in the notebook._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Run the predictions here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "_Is your model able to perform equally well on captured pictures when compared to testing on the dataset?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Visualize the softmax probabilities here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "*Use the model's softmax probabilities to visualize the **certainty** of its predictions, [`tf.nn.top_k`](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#top_k) could prove helpful here. Which predictions is the model certain of? Uncertain? If the model was incorrect in its initial prediction, does the correct prediction appear in the top k? (k should be 5 at most)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "_If necessary, provide documentation for how an interface was built for your model to load and classify newly-acquired images._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
