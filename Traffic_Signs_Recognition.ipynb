{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier\n",
    "\n",
    "In this notebook, a template is provided for you to implement your functionality in stages which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission, if necessary. Sections that begin with **'Implementation'** in the header indicate where you should begin your implementation for your project. Note that some sections of implementation are optional, and will be marked with **'Optional'** in the header.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Exploration\n",
    "\n",
    "Visualize the German Traffic Signs Dataset. This is open ended, some suggestions include: plotting traffic signs images, plotting the count of each sign, etc. Be creative!\n",
    "\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- features -> the images pixel values, (width, height, channels)\n",
    "- labels -> the label of the traffic sign\n",
    "- sizes -> the original width and height of the image, (width, height)\n",
    "- coords -> coordinates of a bounding box around the sign in the image, (x1, y1, x2, y2). Based the original image (not the resized version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg', warn = False)\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_pickled_data(file, columns):\n",
    "    \"\"\"\n",
    "    Loads pickled training and test data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file    : \n",
    "              Name of the pickle file.\n",
    "    columns : list of strings, optional, defaults to `['features', 'labels']`\n",
    "              List of columns in pickled data we're interested in.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of datasets for given columns.    \n",
    "    \"\"\"\n",
    "\n",
    "    with open(file, mode='rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    \n",
    "    return tuple(map(lambda c: dataset[c], columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.io.parsers import read_csv\n",
    "\n",
    "signnames = read_csv(\"signnames.csv\").values[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset_file = \"traffic-signs-data/train.p\"\n",
    "test_dataset_file = \"traffic-signs-data/test.p\"\n",
    "\n",
    "X_train, y_train, sizes, positions = load_pickled_data(train_dataset_file, ['features', 'labels', 'sizes', 'coords'])\n",
    "X_test, y_test = load_pickled_data(test_dataset_file, ['features', 'labels'])\n",
    "\n",
    "n_train = y_train.shape[0]\n",
    "n_test = y_test.shape[0]\n",
    "image_shape = X_train[0].shape\n",
    "image_size = image_shape[0]\n",
    "sign_classes, class_indices, class_counts = np.unique(y_train, return_index = True, return_counts = True)\n",
    "n_classes = class_counts.shape[0]\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original photos\n",
    "\n",
    "Let's first explore data about original photos. Who knows — maybe we can spot some data correlations with signs positions in the photos or positions across specific sign classes.\n",
    "\n",
    "Let's first plot histograms of original photos' sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "width_fraction = (positions[:, 2] - positions[:, 0]) / sizes[:, 0]\n",
    "height_fraction = (positions[:, 3] - positions[:, 1]) / sizes[:, 1]\n",
    "sizes_figure = pyplot.figure(figsize = (10, 4))\n",
    "\n",
    "axis = sizes_figure.add_subplot(1, 2, 1)\n",
    "axis.hist(sizes[:, 0])\n",
    "pyplot.title('Original photos\\'s widths')\n",
    "pyplot.xlabel('Width')\n",
    "pyplot.ylabel('Photos')\n",
    "\n",
    "axis = sizes_figure.add_subplot(1, 2, 2)\n",
    "axis.hist(sizes[:, 1])\n",
    "pyplot.title('Original photos\\'s heights')\n",
    "pyplot.xlabel('Height')\n",
    "pyplot.ylabel('Photos')\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't look too promising — apparently photos are of all different sizes, majority ranging from ~30 to ~150 px both width and height. Actually both histograms looks surprisingly similar, I bet aspect ratios of all photos are close to 1. Let's check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sizes_figure = pyplot.figure()\n",
    "axis = sizes_figure.add_subplot(1, 1, 1)\n",
    "axis.hist(sizes[:, 0]/sizes[:, 1])\n",
    "pyplot.title('Photos\\' aspect ratios distribution')\n",
    "pyplot.xlabel('Aspect ratio')\n",
    "pyplot.ylabel('Photos')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go! Vast majority of original photos had aspect ratio ranging from 0.9 to 1.2, e.g. almost all photos were more or less square.\n",
    "\n",
    "Let's check how much area of the original image did signs occupy? In other words, to which extent did we have to crop the original photo to get the sign out of it. Again, let's check for both width and height by calculating sign dimensions as fractions of original photo dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "width_fraction = (positions[:, 2] - positions[:, 0]) / sizes[:, 0]\n",
    "height_fraction = (positions[:, 3] - positions[:, 1]) / sizes[:, 1]\n",
    "\n",
    "sizes_figure = pyplot.figure(figsize = (10, 4))\n",
    "\n",
    "axis = sizes_figure.add_subplot(1, 2, 1)\n",
    "axis.hist(width_fraction)\n",
    "pyplot.title('Sign width / original photo width')\n",
    "pyplot.xlabel('Width fraction')\n",
    "pyplot.ylabel('Photos')\n",
    "\n",
    "axis = sizes_figure.add_subplot(1, 2, 2)\n",
    "axis.hist(height_fraction)\n",
    "pyplot.title('Sign height / original photo height')\n",
    "pyplot.xlabel('Height fraction')\n",
    "pyplot.ylabel('Photos')\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def print_stats(array, label):\n",
    "    _, minmax, mean, variance, _, _ = stats.describe(array)\n",
    "    margin = int((40 - len(label)) / 2)\n",
    "    for i in range(margin * 2): \n",
    "        if i == margin: print(label.upper(), end=\"\")\n",
    "        print(\"=\", end=\"\")    \n",
    "    print()\n",
    "\n",
    "    print(\"  %-*s %s\" % (15, \"Min:\", str(minmax[0])))\n",
    "    print(\"  %-*s %s\" % (15, \"Max:\", str(minmax[1])))\n",
    "    print(\"  %-*s %s\" % (15, \"Mean:\", str(mean)))\n",
    "    print(\"  %-*s %s\" % (15, \"Variance:\", str(variance)))\n",
    "    \n",
    "    print()\n",
    "    \n",
    "print_stats(width_fraction, \" sign_width / photo_width \")\n",
    "print_stats(height_fraction, \" sign_height / photo_height \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really promising either. All photos had almost nothing but signs in them, for both width and height signs occupied at least half of the dimension, and for majority of samples signs occupied around 75% of the photo.\n",
    "\n",
    "Now, if we found any uneven distributions in the dataset I would be tempted to try and analyze it on per-class basis. Although for now data looks pretty homogeneous and I doubt there will be any meaningful patterns related to specific classes based on the original samples' images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample images\n",
    "\n",
    "Ok, let's check out the sign samples. We will go through all dataset classes, noting number of samples and plotting 10 random images representing each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "col_width = max(len(name) for name in signnames)\n",
    "\n",
    "for c, c_index, c_count in zip(sign_classes, class_indices, class_counts):\n",
    "    print(\"Class %i: %-*s  %s samples\" % (c, col_width, signnames[c], str(c_count)))\n",
    "    fig = pyplot.figure(figsize = (6, 1))\n",
    "    fig.subplots_adjust(left = 0, right = 1, bottom = 0, top = 1, hspace = 0.05, wspace = 0.05)\n",
    "    random_indices = random.sample(range(c_index, c_index + c_count), 10)\n",
    "    for i in range(10):\n",
    "        axis = fig.add_subplot(1, 10, i + 1, xticks=[], yticks=[])\n",
    "        axis.imshow(X_train[random_indices[i]])\n",
    "    pyplot.show()\n",
    "    print(\"--------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bad news**: Some classes are highly underrepresented. Overall amount of data is disappointing: some classes have as little as 210 samples, this won't be enough for most of the models to generalise well.\n",
    "\n",
    "**Good news**: There is a room for data augmentation.\n",
    "\n",
    "Images with some signs are horizontally symmetrical (like **Bumpy road** or **Ahead only**), and can be simply flipped horizontally, thus allowing us to get twice as much data for these classes. \n",
    "\n",
    "Other signs come in kind of interchageable pairs, like **Keep right** and **Keep left**: those signs can be flipped and assigned to a paired class. In some cases (like **Keep right** and **Keep left**) we increase the number of samples for **Keep left** from 300 to 2370!\n",
    "\n",
    "CNNs have built-in invariance to small translations, scaling and rotations. The training set doesn't seem to contain those deformations, so we will add those in our data augmentation step as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "Design and implement a deep learning model that learns to recognize traffic signs. Train and test your model on the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "\n",
    "- Your model can be derived from a deep feedforward net or a deep convolutional network.\n",
    "- Play around preprocessing techniques (normalization, rgb to grayscale, etc)\n",
    "- Number of examples per label (some have more than others).\n",
    "- Generate fake data.\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf). It's not required to be familiar with the approach used in the paper but, it's good practice to try to read papers like these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def preprocess_dataset(X, y = None, balanced_classes = False):\n",
    "    \"\"\"\n",
    "    Performs feature scaling, one-hot encoding of labels and shuffles the data if labels are provided.\n",
    "    Optionally makes the classes representation in the dataset balanced by dupllicating existing examples\n",
    "    to match the most represented class in numbers.\n",
    "    Assumes original dataset is sorted by labels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X                : ndarray\n",
    "                       Dataset array containing feature examples.\n",
    "    y                : ndarray, optional, defaults to `None`\n",
    "                       Dataset labels in index form.\n",
    "    balanced_classes : bool, optional, defaults to `False`\n",
    "                       Flag indicating if we should balance the classes in the dataset.\n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of X and y.    \n",
    "    \"\"\"\n",
    "        \n",
    "    # Convert to grayscale, e.g. single channel Y\n",
    "    X = 0.2125 * X[:, :, :, 0] + 0.7154 * X[:, :, :, 1] + 0.0721 * X[:, :, :, 2]\n",
    "    # Scale features to be in [0, 1]\n",
    "    X = (X / 255.).astype(np.float32)\n",
    "    \n",
    "    sign_classes, class_indices, class_counts = np.unique(y, return_index = True, return_counts = True)\n",
    "    max_c = max(class_counts)\n",
    "    num_c = class_counts.shape[0]\n",
    "    \n",
    "    if balanced_classes:\n",
    "        # Duplicate images for underrepresented classes to match the most represented class.\n",
    "        X_balanced = np.zeros([max_c * num_c, X.shape[1], X.shape[2]])\n",
    "        y_balanced = np.zeros([max_c * num_c], dtype = int)\n",
    "        for c, c_index, c_count in zip(sign_classes, class_indices, class_counts):\n",
    "            for i in range(max_c // c_count):\n",
    "                start = c * max_c + c_count * i\n",
    "                end = start + c_count\n",
    "                X_balanced[start : end, :, :] = X[c_index : c_index + c_count, :, :]\n",
    "            start = c * max_c + c_count * (max_c // c_count)\n",
    "            end = start + max_c % c_count\n",
    "            X_balanced[start : end, :, :] = X[c_index : c_index + max_c % c_count, :, :]\n",
    "            y_balanced[c * max_c : (c + 1) * max_c] = c\n",
    "\n",
    "        X = X_balanced\n",
    "        y = y_balanced\n",
    "        \n",
    "    if y is not None:  \n",
    "        # Convert to one-hot encoding. Convert back with `y = y.nonzero()[1]`\n",
    "        y = np.eye(num_c)[y]\n",
    "        X, y = shuffle(X, y, random_state = 42)\n",
    "\n",
    "    # Add a single grayscale channel\n",
    "    X = X.reshape(X.shape + (1,)) \n",
    "    return X, y\n",
    "\n",
    "def class_name(one_hot):\n",
    "    return signnames[one_hot.nonzero()[0][0]]\n",
    "\n",
    "def load_and_process_data(pickled_data_file):\n",
    "    \"\"\"\n",
    "    Loads pickled data and preprocesses images and labels by scaling features, \n",
    "    shuffling the data and applying one-hot encoding to labels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pickled_data_file  : string\n",
    "                         Pickled data file name.\n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of X and y containing preloaded and preprocessed features and labels respectively.    \n",
    "    \"\"\"\n",
    "    X, y = load_pickled_data(pickled_data_file, columns = ['features', 'labels'])\n",
    "    X, y = preprocess_dataset(X, y)\n",
    "    return (X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "\n",
    "_Describe the techniques used to preprocess the data._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "I convert images to grayscale, scale to `[0, 1]` and shuffle the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "_Describe how you set up the training, validation and testing data for your model. If you generated additional data, why?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "I decided to use a 20% slice of training data for validation. Normally I would split entire dataset as 70% training, 15% validation, 15% testing, but since here test dataset was supplied separately I assumed it's better to use all of it solely for testing.\n",
    "\n",
    "The dataset looked quite small and some classes were significantly less represented than others, so I reckon the model wouldn't generalise well without additional examples. I decided to use data augmentation by applying randomised transformations while supplying batches of training data to the model. I find Lasagne's `BatchIterator` class very convenient, as one can simply override the `transform()` function and provide their data augmentation pipeline there.\n",
    "\n",
    "So that's what I did, my pipeline includes:\n",
    "* **Rotation**. Rotates the image at a random angle in the range of `[-15, 15]` degrees.\n",
    "* **Projection transform**. Warps the image by projecting each of the image corners to a random point within a 10% of image size distance from the original point.\n",
    "* **Gamma adjustment**. Adjusts image brightness/contrast by applying gamma adjustment in the range of `[0.5, 1.5]`.\n",
    "* **Blur**. Applies gaussian blur with sigma in the range of `[0, 0.8]`.\n",
    "* **Noise**. Adds random noise to the image. I've switched it off for now, but might experiment with it later.\n",
    "* Last but not least, **horizontal flip**. Flips images horizontally if their class would remain the same, *OR* flips and updates the label, if flipped image represents some other class (e.g. flipped *Turn left* sign would become *Turn right*).\n",
    "\n",
    "I chose these transformations as they seemed to be close to what we could see on the photos. Image could be blurred and/or noisy due to low resolution source photo, it can be either too bright or too dark depending on the outside conditions, and it can look rotated and/or projection warped on the photo depending on the shooting angle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Struct to organise some of the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Parameters = namedtuple('Parameters', [\n",
    "        # Data parameters\n",
    "        'num_classes', 'image_size', 'valid_size',\n",
    "        # Training parameters\n",
    "        'batch_size', 'max_epochs', 'log_epoch', \n",
    "        # Optimisations\n",
    "        'data_augmentation', 'learning_rate_decay', 'learning_rate', 'momentum_increase', \n",
    "        'dropout_enabled', 'l2_reg_enabled', 'l2_lambda', 'stage2', 'early_stopping_enabled', \n",
    "        'early_stopping_patience', 'resume_training', \n",
    "        # Layers architecture\n",
    "        'conv1_k', 'conv1_d', 'conv1_p', \n",
    "        'conv2_k', 'conv2_d', 'conv2_p', \n",
    "        'conv3_k', 'conv3_d', 'conv3_p', \n",
    "        'fc4_size', 'fc4_p', \n",
    "        'fc5_size', 'fc5_p'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "def get_time_hhmmss(start):\n",
    "    \"\"\"\n",
    "    Calculates time since `start` and formats as a string.\n",
    "    \"\"\"\n",
    "    end = time.time()\n",
    "    m, s = divmod(end - start, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    time_str = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "    return time_str\n",
    "\n",
    "class Paths(object):\n",
    "    \"\"\"\n",
    "    Provides easy access to common paths we use for persisting \n",
    "    the data associated with model training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        \"\"\"\n",
    "        Initialises a new `Paths` instance and creates corresponding folders if needed.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params  : Parameters\n",
    "                  Structure (`namedtuple`) containing model parameters.\n",
    "        \"\"\"\n",
    "        self.model_name = self.get_model_name(params)\n",
    "        self.root_path = os.getcwd() + \"/models/\" + self.model_name + \"/\"\n",
    "        os.makedirs(self.root_path, exist_ok = True)\n",
    "\n",
    "    def get_model_name(self, params):\n",
    "        \"\"\"\n",
    "        Generates a model name with some of the crucial model parameters encoded into the name.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params  : Parameters\n",
    "                  Structure (`namedtuple`) containing model parameters.\n",
    "                  \n",
    "        Returns\n",
    "        -------\n",
    "        Model name.\n",
    "        \"\"\"\n",
    "        # We will encode model settings in its name: architecture, optimisations applied, etc.\n",
    "        model_name = \"k{}d{}p{}_k{}d{}p{}_k{}d{}p{}_fc{}p{}_fc{}p{}\".format(\n",
    "            params.conv1_k, params.conv1_d, params.conv1_p, \n",
    "            params.conv2_k, params.conv2_d, params.conv2_p, \n",
    "            params.conv3_k, params.conv3_d, params.conv3_p, \n",
    "            params.fc4_size, params.fc4_p, \n",
    "            params.fc5_size, params.fc5_p\n",
    "        )\n",
    "        model_name += \"_st2\" if params.stage2 else \"_no-st2\"\n",
    "        model_name += \"_aug\" if params.data_augmentation else \"_no-aug\"\n",
    "        model_name += \"_lrdec\" if params.learning_rate_decay else \"_no-lrdec\"\n",
    "        model_name += \"_mominc\" if params.momentum_increase else \"_no-mom\"\n",
    "        model_name += \"_dr\" if params.dropout_enabled else \"_no-dr\"\n",
    "        model_name += \"_l2\" if params.l2_reg_enabled else \"_no-l2\"\n",
    "        return model_name\n",
    "\n",
    "    def model_path(self):\n",
    "        \"\"\"\n",
    "        Generates path to the model file.\n",
    "   \n",
    "        Returns\n",
    "        -------\n",
    "        Model file path.\n",
    "        \"\"\"\n",
    "        return self.root_path + \"model.ckpt\"\n",
    "\n",
    "    def train_history_path(self):\n",
    "        \"\"\"\n",
    "        Generates path to the train history file.\n",
    "   \n",
    "        Returns\n",
    "        -------\n",
    "        Train history file path.\n",
    "        \"\"\"\n",
    "        return self.root_path + \"train_history\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    \"\"\"\n",
    "    Provides early stopping functionality. Keeps track of model accuracy, \n",
    "    and if it doesn't improve over time restores last best performing \n",
    "    parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, saver, session, patience = 100):\n",
    "        \"\"\"\n",
    "        Initialises a `EarlyStopping` isntance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        saver     : \n",
    "                    TensorFlow Saver object to be used for saving and restoring model.\n",
    "        session   : \n",
    "                    TensorFlow Session object containing graph where model is restored.\n",
    "        patience  : \n",
    "                    Early stopping patience. This is the number of epochs we wait for \n",
    "                    accuracy to start improving again before stopping and restoring \n",
    "                    previous best performing parameters.\n",
    "                  \n",
    "        Returns\n",
    "        -------\n",
    "        New instance.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.saver = saver\n",
    "        self.session = session\n",
    "        self.best_accuracy = 0.\n",
    "        self.best_accuracy_epoch = 0\n",
    "        self.restore_path = None\n",
    "\n",
    "    def __call__(self, accuracy, epoch):\n",
    "        \"\"\"\n",
    "        Checks if we need to stop and restores the last well performing values if we do.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        accuracy  : \n",
    "                    Last epoch accuracy.\n",
    "        epoch     : \n",
    "                    Last epoch number.\n",
    "                  \n",
    "        Returns\n",
    "        -------\n",
    "        `True` if we waited enough and it's time to stop and we restored the \n",
    "        best performing weights, or `False` otherwise.\n",
    "        \"\"\"\n",
    "        if accuracy > self.best_accuracy:\n",
    "            self.best_accuracy = accuracy\n",
    "            self.best_accuracy_epoch = epoch\n",
    "            self.restore_path = self.saver.save(self.session, os.getcwd() + \"/early_stopping_checkpoint\")\n",
    "        elif self.best_accuracy_epoch + self.patience < epoch:\n",
    "            print(\"Early stopping.\\nBest monitored accuracy was {:.3f}%% at epoch {}.\".format(\n",
    "                self.best_accuracy, self.best_accuracy_epoch\n",
    "            ))\n",
    "            if self.restore_path != None:\n",
    "                self.saver.restore(self.session, self.restore_path)\n",
    "            else:\n",
    "                print(\"ERROR: Failed to restore session\")\n",
    "            return True\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/bin/anaconda3/lib/python3.5/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n",
      "/home/alex/bin/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from nolearn.lasagne import BatchIterator\n",
    "from skimage.transform import rotate\n",
    "from skimage.transform import warp\n",
    "from skimage.transform import ProjectiveTransform\n",
    "from skimage.util import random_noise\n",
    "from skimage.exposure import adjust_gamma\n",
    "from skimage.filters import gaussian\n",
    "import random\n",
    "\n",
    "class AugmentedSignsBatchIterator(BatchIterator):\n",
    "    \"\"\"\n",
    "    Iterates over dataset in batches. \n",
    "    Allows images augmentation by randomly rotating, applying projection, \n",
    "    adjusting gamma, blurring, adding noize and flipping horizontally.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Classes of signs that, when flipped horizontally, should still be classified as the same class\n",
    "    self_flippable = np.array([11, 12, 13, 15, 17, 18, 22, 26, 30, 35])\n",
    "    # Classes of signs that, when flipped horizontally, should still be classified as some other class\n",
    "    cross_flippable = np.array([\n",
    "        [19, 20], \n",
    "        [33, 34], \n",
    "        [36, 37], \n",
    "        [38, 39]\n",
    "    ])\n",
    "      \n",
    "    def __init__(self, batch_size, shuffle = False, seed = 42):\n",
    "        super(AugmentedSignsBatchIterator, self).__init__(batch_size, shuffle, seed)\n",
    "        \n",
    "        # Total number of classes    \n",
    "        num_classes = 43\n",
    "            \n",
    "        # Generate a conversion matrix for the one-hot encoded `y` when flipping the image.\n",
    "        self.y_flip_conversion = np.zeros((num_classes, num_classes))\n",
    "        self.y_flip_conversion[self.self_flippable, self.self_flippable] = 1.\n",
    "        self.y_flip_conversion[self.cross_flippable[:, 0], self.cross_flippable[:, 1]] = 1.\n",
    "        self.y_flip_conversion[self.cross_flippable[:, 1], self.cross_flippable[:, 0]] = 1.\n",
    "        \n",
    "        # Create a list of all flippable classes.\n",
    "        self.flippable = np.append(self.self_flippable, self.cross_flippable.flatten())\n",
    "\n",
    "    def transform(self, Xb, yb):\n",
    "        \"\"\"\n",
    "        Applies a pipeline of randomised transformations for data augmentation.\n",
    "        \"\"\"\n",
    "        Xb, yb = super(AugmentedSignsBatchIterator, self).transform(\n",
    "            Xb if yb is None else Xb.copy(), \n",
    "            None if yb is None else yb.copy()\n",
    "        )\n",
    "        \n",
    "        if yb is not None:\n",
    "            batch_size = Xb.shape[0]\n",
    "            image_size = Xb.shape[1]\n",
    "            \n",
    "            Xb = self.rotate(Xb, batch_size)\n",
    "            Xb = self.apply_projection_transform(Xb, batch_size, image_size)\n",
    "            Xb = self.adjust_gamma(Xb, batch_size)\n",
    "            Xb = self.blur(Xb, batch_size)\n",
    "            #Xb = self.add_noise(Xb, batch_size)\n",
    "            Xb, yb = self.flip_horizontally(Xb, yb, batch_size)\n",
    "\n",
    "        return Xb, yb\n",
    "    \n",
    "    def flip_horizontally(self, Xb, yb, batch_size, p = 0.5):\n",
    "        \"\"\"\n",
    "        Flips random subset of images horizontally, but only those that can either be flipped \n",
    "        and would still represent the same class, or those that can represent some other class\n",
    "        when flipped. In latter case updates batch labels as well.\n",
    "        \"\"\"\n",
    "        # Get a mask of randomly chosen half of elements\n",
    "        random_choice_indices = np.random.choice(batch_size, int(batch_size * p), replace = False)\n",
    "        random_choice_mask = np.zeros(batch_size, dtype = bool)\n",
    "        random_choice_mask[random_choice_indices] = True\n",
    "        \n",
    "        # Get a mask of flippable elements in current batch\n",
    "        flippable_mask = np.in1d(yb.nonzero()[1], self.flippable)\n",
    "        \n",
    "        # Get an intersection mask of randomly chosen AND flippable elements\n",
    "        flip_mask = random_choice_mask & flippable_mask\n",
    "        \n",
    "        # Flip images and flip labels as well, if requried. The `y_flip_conversion` is going to take care of that.\n",
    "        Xb[flip_mask] = Xb[flip_mask, :, ::-1, :]\n",
    "        yb[flip_mask] = np.transpose(np.dot(self.y_flip_conversion, np.transpose(yb[flip_mask])))\n",
    "        \n",
    "        return Xb, yb\n",
    "        \n",
    "    def rotate(self, Xb, batch_size, p = 0.5):\n",
    "        \"\"\"\n",
    "        Applies random rotation in range (-15, 15) degrees to a random subset of images.\n",
    "        \"\"\"\n",
    "        for i in np.random.choice(batch_size, int(batch_size * p), replace = False):\n",
    "            Xb[i] = rotate(Xb[i], random.uniform(-15., 15.), mode = 'edge')\n",
    "        return Xb \n",
    "    \n",
    "    def add_noise(self, Xb, batch_size, p = 0.5):\n",
    "        \"\"\"\n",
    "        Adds random noise to a random subset of images.\n",
    "        \"\"\"\n",
    "        modes = [\"gaussian\", \"localvar\", \"poisson\", \"speckle\"] # Excluding \"salt\", \"pepper\", \"s&p\" for now\n",
    "        for i in np.random.choice(batch_size, int(batch_size * p), replace = False):\n",
    "            Xb[i] = random_noise(Xb[i], mode = random.choice(modes))\n",
    "        return Xb\n",
    "    \n",
    "    def adjust_gamma(self, Xb, batch_size, p = 0.5):\n",
    "        \"\"\"\n",
    "        Applies gamma adjustment to a random subset of images, updating gamma with a random value in the (0.5, 1.5) range.\n",
    "        \"\"\"\n",
    "        for i in np.random.choice(batch_size, int(batch_size * p), replace = False):\n",
    "            Xb[i] = adjust_gamma(Xb[i], gamma = random.uniform(0.5, 1.5))\n",
    "        return Xb        \n",
    "\n",
    "    def blur(self, Xb, batch_size, p = 0.5):\n",
    "        \"\"\"\n",
    "        Applies gaussian blur with a random sigma in the (0, 0.8) range to a random subset of images.\n",
    "        \"\"\"\n",
    "        for i in np.random.choice(batch_size, int(batch_size * p), replace = False):\n",
    "            Xb[i] = gaussian(Xb[i], sigma = random.uniform(0., 0.8))\n",
    "        return Xb   \n",
    "    \n",
    "    def apply_projection_transform(self, Xb, batch_size, image_size, p = 0.5):\n",
    "        \"\"\"\n",
    "        Applies projection transform to a random subset of images. Projection margins are randomised in a range\n",
    "        of (-10%, +10%) of the image size.\n",
    "        \"\"\"\n",
    "        d = image_size * 0.1\n",
    "        for i in np.random.choice(batch_size, int(batch_size * p), replace = False):        \n",
    "            tl_top = random.uniform(-d, d)     # Top left corner, top margin\n",
    "            tl_left = random.uniform(-d, d)    # Top left corner, left margin\n",
    "            bl_bottom = random.uniform(-d, d)  # Bottom left corner, bottom margin\n",
    "            bl_left = random.uniform(-d, d)    # Bottom left corner, left margin\n",
    "            tr_top = random.uniform(-d, d)     # Top right corner, top margin\n",
    "            tr_right = random.uniform(-d, d)   # Top right corner, right margin\n",
    "            br_bottom = random.uniform(-d, d)  # Bottom right corner, bottom margin\n",
    "            br_right = random.uniform(-d, d)   # Bottom right corner, right margin\n",
    "\n",
    "            transform = ProjectiveTransform()\n",
    "            transform.estimate(np.array((\n",
    "                    (tl_left, tl_top),\n",
    "                    (bl_left, image_size - bl_bottom),\n",
    "                    (image_size - br_right, image_size - br_bottom),\n",
    "                    (image_size - tr_right, tr_top)\n",
    "                )), np.array((\n",
    "                    (0, 0),\n",
    "                    (0, image_size),\n",
    "                    (image_size, image_size),\n",
    "                    (image_size, 0)\n",
    "                )))\n",
    "            Xb[i] = warp(Xb[i], transform, output_shape=(image_size, image_size), mode = 'edge')\n",
    "\n",
    "        return Xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_iterator = AugmentedSignsBatchIterator(batch_size = 20)\n",
    "for x_batch, y_batch in batch_iterator(X_train, y_train):\n",
    "    for i in range(20): \n",
    "        print(\"Source class: \" + class_name(y_train[i]) + \"\\n Batch class: \" + class_name(y_batch[i]))\n",
    "        # plot two images:\n",
    "        fig = pyplot.figure(figsize=(3, 1))\n",
    "        axis = fig.add_subplot(1, 2, 1, xticks=[], yticks=[])\n",
    "        axis.imshow(X_train[i].reshape(image_size, image_size), cmap='gray')\n",
    "        axis = fig.add_subplot(1, 2, 2, xticks=[], yticks=[])\n",
    "        axis.imshow(x_batch[i].reshape(image_size, image_size), cmap='gray')\n",
    "        pyplot.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logger to keep track of the training even if no browser is connected to kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    \"\"\"\n",
    "    Duplicates console logs into a text file.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_path):\n",
    "        self.path = root_path + \"log.txt\"\n",
    "        os.makedirs(root_path, exist_ok = True)\n",
    "\n",
    "    def __call__(self, string):\n",
    "        \"\"\"\n",
    "        Logs the value to console and appends the same string to the log file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        string  : string\n",
    "                  Value to be logged.\n",
    "        \"\"\"\n",
    "        print(string)\n",
    "        with open(self.path, 'a') as file:\n",
    "            file.write(string + '\\n')\n",
    "            \n",
    "    def clear(self):\n",
    "        \"\"\"\n",
    "        Removes log file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            os.remove(self.path)\n",
    "        except OSError:\n",
    "            pass    \n",
    "\n",
    "    def log_parameters(self, params, train_size, valid_size, test_size):\n",
    "        \"\"\"\n",
    "        Logs model parameters to console and appends the same text representation to the log file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params    : Parameters\n",
    "                    Structure (`namedtuple`) containing model parameters.\n",
    "        train_size: int\n",
    "                    Size of the training dataset.\n",
    "        valid_size: int\n",
    "                    Size of the training dataset.\n",
    "        test_size : int\n",
    "                    Size of the training dataset.\n",
    "        \"\"\"\n",
    "        if params.resume_training:\n",
    "            self(\"=================================================\")\n",
    "            self(\"=============== RESUMING TRAINING ===============\")\n",
    "            self(\"=================================================\")\n",
    "\n",
    "        self(\"===================== DATA ======================\")\n",
    "        self(\"            Training set: {} examples\".format(train_size))\n",
    "        self(\"          Validation set: {} examples\".format(valid_size))\n",
    "        self(\"             Testing set: {} examples\".format(test_size))\n",
    "        self(\"              Batch size: {}\".format(params.batch_size))   \n",
    "\n",
    "        self(\"===================== MODEL =====================\")\n",
    "        self(\"----------------- ARCHITECTURE ------------------\")  \n",
    "        self(\" %-*s %-*s %-*s %-*s\" % (10, \"\", 10, \"Type\", 8, \"Size\", 15, \"Dropout (keep p)\"))    \n",
    "        self(\" %-*s %-*s %-*s %-*s\" % (10, \"Layer 1\", 10, \"{}x{} Conv\".format(params.conv1_k, params.conv1_k), 8, str(params.conv1_d), 15, str(params.conv1_p)))    \n",
    "        self(\" %-*s %-*s %-*s %-*s\" % (10, \"Layer 2\", 10, \"{}x{} Conv\".format(params.conv2_k, params.conv2_k), 8, str(params.conv2_d), 15, str(params.conv2_p)))    \n",
    "        self(\" %-*s %-*s %-*s %-*s\" % (10, \"Layer 3\", 10, \"{}x{} Conv\".format(params.conv3_k, params.conv3_k), 8, str(params.conv3_d), 15, str(params.conv3_p)))    \n",
    "        self(\" %-*s %-*s %-*s %-*s\" % (10, \"Layer 4\", 10, \"FC\", 8, str(params.fc4_size), 15, str(params.fc4_p)))    \n",
    "        self(\" %-*s %-*s %-*s %-*s\" % (10, \"Layer 5\", 10, \"FC\", 8, str(params.fc5_size), 15, str(params.fc5_p))) \n",
    "        self(\"------------------ PARAMETERS -------------------\")\n",
    "        self(\"       Data augmentation: \" + (\"Enabled\" if params.data_augmentation else \"Disabled\"))\n",
    "        self(\"     Learning rate decay: \" + (\"Enabled\" if params.learning_rate_decay else \"Disabled (rate = {})\".format(params.learning_rate)))\n",
    "        self(\"       Momentum increase: \" + (\"Enabled\" if params.momentum_increase else \"Disabled\"))\n",
    "        self(\"                 Dropout: \" + (\"Enabled\" if params.dropout_enabled else \"Disabled\"))\n",
    "        self(\"       L2 Regularization: \" + (\"Enabled (lambda = {})\".format(params.l2_lambda) if params.l2_reg_enabled else \"Disabled\"))\n",
    "        self(\"      2nd Stage training: \" + (\"Enabled\" if params.stage2 else \"Disabled\"))\n",
    "        self(\"          Early stopping: \" + (\"Enabled (patience = {})\".format(params.early_stopping_patience) if params.early_stopping_enabled else \"Disabled\"))\n",
    "        self(\" Keep training old model: \" + (\"Enabled\" if params.resume_training else \"Disabled\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(input, size):\n",
    "    \"\"\"\n",
    "    Performs a single fully connected layer pass, e.g. returns `input * weights + bias`.\n",
    "    \"\"\"\n",
    "    weights = tf.get_variable( 'weights', \n",
    "        shape = [input.get_shape()[1], size],\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "      )\n",
    "    biases = tf.get_variable( 'biases',\n",
    "        shape = [size],\n",
    "        initializer = tf.constant_initializer(0.0)\n",
    "      )\n",
    "    return tf.matmul(input, weights) + biases\n",
    "\n",
    "def conv_relu(input, kernel_size, depth):\n",
    "    \"\"\"\n",
    "    Performs a single convolution layer pass.\n",
    "    \"\"\"\n",
    "    weights = tf.get_variable( 'weights', \n",
    "        shape = [kernel_size, kernel_size, input.get_shape()[3], depth],\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "      )\n",
    "    biases = tf.get_variable( 'biases',\n",
    "        shape = [depth],\n",
    "        initializer = tf.constant_initializer(0.0)\n",
    "      )\n",
    "    conv = tf.nn.conv2d(input, weights,\n",
    "        strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "def pool(input, size):\n",
    "    \"\"\"\n",
    "    Performs a max pooling layer pass.\n",
    "    \"\"\"\n",
    "    return tf.nn.max_pool(\n",
    "        input, \n",
    "        ksize = [1, size, size, 1], \n",
    "        strides = [1, size, size, 1], \n",
    "        padding = 'SAME'\n",
    "    )\n",
    "\n",
    "def model_pass(input, params):\n",
    "    \"\"\"\n",
    "    Performs a full model pass.\n",
    "    \"\"\"\n",
    "    # Convolutions\n",
    "    \n",
    "    with tf.variable_scope('conv1'):\n",
    "        conv1 = conv_relu(input, kernel_size = params.conv1_k, depth = params.conv1_d) \n",
    "    with tf.variable_scope('pool1'): \n",
    "        pool1 = pool(conv1, size = 2)\n",
    "        if params.dropout_enabled: pool1 = tf.nn.dropout(pool1, keep_prob = params.conv1_p)\n",
    "    with tf.variable_scope('conv2'):\n",
    "        conv2 = conv_relu(pool1, kernel_size = params.conv2_k, depth = params.conv2_d)\n",
    "    with tf.variable_scope('pool2'):\n",
    "        pool2 = pool(conv2, size = 2)\n",
    "        if params.dropout_enabled: pool2 = tf.nn.dropout(pool2, keep_prob = params.conv2_p)\n",
    "    with tf.variable_scope('conv3'):\n",
    "        conv3 = conv_relu(pool2, kernel_size = params.conv3_k, depth = params.conv3_d)\n",
    "    with tf.variable_scope('pool3'):\n",
    "        pool3 = pool(conv3, size = 2)\n",
    "        if params.dropout_enabled: pool3 = tf.nn.dropout(pool3, keep_prob = params.conv3_p)\n",
    "    \n",
    "    # Fully connected\n",
    "    \n",
    "    # 1st stage output\n",
    "    shape = pool3.get_shape().as_list()\n",
    "    pool3 = tf.reshape(pool3, [-1, shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    if params.stage2:\n",
    "        # 2nd stage output\n",
    "        shape = pool1.get_shape().as_list()\n",
    "        pool1 = tf.reshape(pool1, [-1, shape[1] * shape[2] * shape[3]])\n",
    "        flattened = tf.concat(1, [pool1, pool3])\n",
    "    else:\n",
    "        flattened = pool3\n",
    "    \n",
    "    with tf.variable_scope('fc4'):\n",
    "        fc4 = fully_connected(flattened, size = params.fc4_size)\n",
    "        if params.dropout_enabled: fc4 = tf.nn.dropout(fc4, keep_prob = params.fc4_p)\n",
    "    with tf.variable_scope('fc5'):\n",
    "        fc5 = fully_connected(fc4, size = params.fc5_size)\n",
    "        if params.dropout_enabled: fc4 = tf.nn.dropout(fc4, keep_prob = params.fc5_p)\n",
    "    with tf.variable_scope('out'):\n",
    "        prediction = fully_connected(fc5, size = params.num_classes)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "_What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.)  For reference on how to build a deep neural network using TensorFlow, see [Deep Neural Network in TensorFlow\n",
    "](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/b516a270-8600-4f93-a0a3-20dfeabe5da6/concepts/83a3a2a2-a9bd-4b7b-95b0-eb924ab14432) from the classroom._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(params, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Performs model training based on provided training dataset \n",
    "    according to provided parameters, and then evaluates trained \n",
    "    model with testing dataset. \n",
    "    Part of the training dataset may be used for validation during\n",
    "    training if specified in model parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params    : Parameters\n",
    "                Structure (`namedtuple`) containing model parameters.\n",
    "    X_train   : \n",
    "                Training dataset. \n",
    "    y_train   : \n",
    "                Training dataset labels. \n",
    "    X_test    : \n",
    "                Testing dataset. \n",
    "    y_test    : \n",
    "                Testing dataset labels. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialisation routines: generate variable scope, create logger, note start time.\n",
    "    paths = Paths(params)\n",
    "    log = Logger(paths.root_path)\n",
    "    start = time.time()\n",
    "    model_variable_scope = paths.model_name\n",
    "\n",
    "    # Prepare data: use portion of training data for validation\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = params.valid_size)\n",
    "    \n",
    "    if not params.resume_training: log.clear()\n",
    "    log.log_parameters(params, y_train.shape[0], y_valid.shape[0], y_test.shape[0]) \n",
    "    \n",
    "    # Build the graph\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Input data. For the training data, we use a placeholder that will be fed at run time with a training minibatch.\n",
    "        tf_x_batch = tf.placeholder(tf.float32, shape = (None, params.image_size[0], params.image_size[1], 1))\n",
    "        tf_y_batch = tf.placeholder(tf.float32, shape = (None, params.num_classes))\n",
    "        current_epoch = tf.Variable(0)  # count the number of epochs\n",
    "\n",
    "        # Model parameters.\n",
    "        if params.learning_rate_decay:\n",
    "            learning_rate = tf.train.exponential_decay(0.03, current_epoch, decay_steps = params.max_epochs, decay_rate = 0.03)\n",
    "        else:\n",
    "            learning_rate = params.learning_rate\n",
    "            \n",
    "        momentum = 0.9 + (0.99 - 0.9) * (current_epoch / params.max_epochs) if params.momentum_increase else 0.99\n",
    "\n",
    "        # Training computation.\n",
    "        with tf.variable_scope(model_variable_scope):\n",
    "            logits = model_pass(tf_x_batch, params)\n",
    "            if params.l2_reg_enabled:\n",
    "                with tf.variable_scope('fc4', reuse = True):\n",
    "                    l2_loss = tf.nn.l2_loss(tf.get_variable('weights')) \n",
    "                with tf.variable_scope('fc5', reuse = True):\n",
    "                    l2_loss += tf.nn.l2_loss(tf.get_variable('weights'))\n",
    "            else:\n",
    "                l2_loss = 0\n",
    "\n",
    "        predictions = tf.nn.softmax(logits)\n",
    "        softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, tf_y_batch)\n",
    "        loss = tf.reduce_mean(softmax_cross_entropy) + params.l2_lambda * l2_loss  \n",
    "\n",
    "        # Optimizer.\n",
    "        optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate = learning_rate\n",
    "        ).minimize(loss)\n",
    "\n",
    "    with tf.Session(graph = graph) as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        # A routine for evaluating current model parameters\n",
    "        def get_accuracy_and_loss_in_batches(X, y):\n",
    "            p = []\n",
    "            sce = []\n",
    "            batch_iterator = BatchIterator(batch_size = 128)\n",
    "            for x_batch, y_batch in batch_iterator(X, y):\n",
    "                [p_batch, sce_batch] = session.run([predictions, softmax_cross_entropy], feed_dict = {\n",
    "                        tf_x_batch : x_batch, \n",
    "                        tf_y_batch : y_batch\n",
    "                    }\n",
    "                )\n",
    "                p.extend(p_batch)\n",
    "                sce.extend(sce_batch)\n",
    "            p = np.array(p)\n",
    "            sce = np.array(sce)\n",
    "            accuracy = 100.0 * np.sum(np.argmax(p, 1) == np.argmax(y, 1)) / p.shape[0]\n",
    "            loss = np.mean(sce)\n",
    "            return (accuracy, loss)\n",
    "        \n",
    "        # If we chose to keep training previously trained model, restore session.\n",
    "        if params.resume_training: tf.train.Saver().restore(session, model_path)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        early_stopping = EarlyStopping(tf.train.Saver(), session, patience = params.early_stopping_patience)\n",
    "        train_loss_history = np.zeros(params.max_epochs)\n",
    "        train_accuracy_history = np.zeros(params.max_epochs)\n",
    "        valid_loss_history = np.zeros(params.max_epochs)\n",
    "        valid_accuracy_history = np.zeros(params.max_epochs)\n",
    "        log(\"=================== TRAINING ====================\")\n",
    "        for epoch in range(params.max_epochs):\n",
    "            current_epoch = epoch\n",
    "            # Train on whole randomised dataset in batches\n",
    "            if params.data_augmentation:\n",
    "                batch_iterator = AugmentedSignsBatchIterator(batch_size = params.batch_size, shuffle = True)  \n",
    "            else: \n",
    "                batch_iterator = BatchIterator(batch_size = params.batch_size, shuffle = True)\n",
    "            for x_batch, y_batch in batch_iterator(X_train, y_train):\n",
    "                session.run([optimizer], feed_dict = {\n",
    "                        tf_x_batch : x_batch, \n",
    "                        tf_y_batch : y_batch\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # If another significant epoch ended, we log our losses.\n",
    "            if (epoch % params.log_epoch == 0):\n",
    "                # Get validation data predictions and log validation loss:\n",
    "                valid_accuracy, valid_loss = get_accuracy_and_loss_in_batches(X_valid, y_valid)\n",
    "                valid_loss_history[epoch] = valid_loss\n",
    "                valid_accuracy_history[epoch] = valid_accuracy\n",
    "\n",
    "                # Get training data predictions and log training loss:\n",
    "                train_accuracy, train_loss = get_accuracy_and_loss_in_batches(X_train, y_train)\n",
    "                train_loss_history[epoch] = train_loss\n",
    "                train_accuracy_history[epoch] = train_accuracy\n",
    "\n",
    "                if (epoch % 1 == 0):\n",
    "                    log(\"---------------- EPOCH %4d/%d ----------------\" % (epoch + 1, params.max_epochs))\n",
    "                    log(\"     Train loss: %.8f, accuracy: %.2f%%\" % (train_loss, train_accuracy))\n",
    "                    log(\"Validation loss: %.8f, accuracy: %.2f%%\" % (valid_loss, valid_accuracy))\n",
    "                    log(\"           Time: \" + get_time_hhmmss(start))\n",
    "\n",
    "            if params.early_stopping_enabled:\n",
    "                # Get validation data predictions and log validation loss:\n",
    "                valid_accuracy, _ = get_accuracy_and_loss_in_batches(X_valid, y_valid)\n",
    "                if early_stopping(valid_accuracy, epoch): break\n",
    "\n",
    "        # Evaluate on test dataset.\n",
    "        test_accuracy, test_loss = get_accuracy_and_loss_in_batches(X_test, y_test)\n",
    "        log(\"=================================================\")\n",
    "        log(\" Test loss: %.8f, accuracy = %.2f%%)\" % (test_loss, test_accuracy)) \n",
    "        log(\" Total time: \" + get_time_hhmmss(start))\n",
    "\n",
    "        # Save model weights for future use.\n",
    "        saved_model_path = saver.save(session, paths.model_path)\n",
    "        log(\"Model file: \" + saved_model_path)\n",
    "        np.savez(paths.train_history_path, train_loss_history = train_loss_history, train_accuracy_history = train_accuracy_history, valid_loss_history = valid_loss_history, valid_accuracy_history = valid_accuracy_history)\n",
    "        log(\"Train history file: \" + paths.train_history_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== DATA ======================\n",
      "            Training set: 31367 examples\n",
      "          Validation set: 7842 examples\n",
      "             Testing set: 12630 examples\n",
      "              Batch size: 128\n",
      "===================== MODEL =====================\n",
      "----------------- ARCHITECTURE ------------------\n",
      "            Type       Size     Dropout (keep p)\n",
      " Layer 1    5x5 Conv   32       0.5            \n",
      " Layer 2    5x5 Conv   64       0.5            \n",
      " Layer 3    5x5 Conv   128      0.5            \n",
      " Layer 4    FC         1024     0.5            \n",
      " Layer 5    FC         1024     0.5            \n",
      "------------------ PARAMETERS -------------------\n",
      "       Data augmentation: Disabled\n",
      "     Learning rate decay: Disabled (rate = 0.001)\n",
      "       Momentum increase: Disabled\n",
      "                 Dropout: Enabled\n",
      "       L2 Regularization: Enabled (lambda = 0.0001)\n",
      "      2nd Stage training: Enabled\n",
      "          Early stopping: Disabled\n",
      " Keep training old model: Disabled\n",
      "=================== TRAINING ====================\n",
      "---------------- EPOCH    1/100 ----------------\n",
      "     Train loss: 0.64347947, accuracy: 82.38%\n",
      "Validation loss: 0.66969812, accuracy: 81.73%\n",
      "           Time: 00:00:06\n",
      "---------------- EPOCH    2/100 ----------------\n",
      "     Train loss: 0.46146476, accuracy: 86.28%\n",
      "Validation loss: 0.51654476, accuracy: 85.46%\n",
      "           Time: 00:00:11\n",
      "---------------- EPOCH    3/100 ----------------\n",
      "     Train loss: 0.36111224, accuracy: 89.66%\n",
      "Validation loss: 0.43088615, accuracy: 88.15%\n",
      "           Time: 00:00:16\n",
      "---------------- EPOCH    4/100 ----------------\n",
      "     Train loss: 0.33201599, accuracy: 91.01%\n",
      "Validation loss: 0.40964413, accuracy: 89.73%\n",
      "           Time: 00:00:21\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-c5cf7801b726>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"traffic-signs-data/test.p\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-00b40cc307a7>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(params, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;31m# Get training data predictions and log training loss:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy_and_loss_in_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                 \u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mtrain_accuracy_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-00b40cc307a7>\u001b[0m in \u001b[0;36mget_accuracy_and_loss_in_batches\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 [p_batch, sce_batch] = session.run([predictions, softmax_cross_entropy], feed_dict = {\n\u001b[1;32m     60\u001b[0m                         \u001b[0mtf_x_batch\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                         \u001b[0mtf_y_batch\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                     }\n\u001b[1;32m     63\u001b[0m                 )\n",
      "\u001b[0;32m/home/alex/bin/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/alex/bin/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/alex/bin/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/alex/bin/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/alex/bin/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "parameters = Parameters(\n",
    "    # Data parameters\n",
    "    num_classes = 43,\n",
    "    image_size = (32, 32),\n",
    "    valid_size = 0.2,\n",
    "    # Training parameters\n",
    "    batch_size = 128,\n",
    "    max_epochs = 100,\n",
    "    log_epoch = 1,\n",
    "    # Optimisations\n",
    "    data_augmentation = False,\n",
    "    learning_rate_decay = False,\n",
    "    learning_rate = 0.001,\n",
    "    momentum_increase = False,\n",
    "    dropout_enabled = True,\n",
    "    l2_reg_enabled = True,\n",
    "    l2_lambda = 0.0001,\n",
    "    stage2 = True,\n",
    "    early_stopping_enabled = False,\n",
    "    early_stopping_patience = 100,\n",
    "    resume_training = False,\n",
    "    # Layers architecture\n",
    "    conv1_k = 5, conv1_d = 32, conv1_p = 0.5,\n",
    "    conv2_k = 5, conv2_d = 64, conv2_p = 0.5,\n",
    "    conv3_k = 5, conv3_d = 128, conv3_p = 0.5,\n",
    "    fc4_size = 1024, fc4_p = 0.5,\n",
    "    fc5_size = 1024, fc5_p = 0.5\n",
    ")\n",
    "\n",
    "# Load and preprocess initial data by scaling features, shuffling the data, balancing classes, etc.\n",
    "X_train, y_train = load_and_process_data(\"traffic-signs-data/train.p\")\n",
    "X_test, y_test = load_and_process_data(\"traffic-signs-data/test.p\")\n",
    "\n",
    "train_model(parameters, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plots history of learning curves for a specific model. You may want to call `pyplot.show()` afterwards.\n",
    "def plot_learning_curves(axis, params, train_column, valid_column, linewidth = 2, train_linestyle = \"b-\", valid_linestyle = \"g-\"):\n",
    "    model_history = np.load(Paths(params).train_history_path + \".npz\")\n",
    "    train_values = model_history[train_column]\n",
    "    valid_values = model_history[valid_column]\n",
    "    epochs = train_values.shape[0]\n",
    "    x_axis = np.arange(epochs)\n",
    "    axis.plot(x_axis[train_values > 0], train_values[train_values > 0], train_linestyle, linewidth=linewidth, label=\"train\")\n",
    "    axis.plot(x_axis[valid_values > 0], valid_values[valid_values > 0], valid_linestyle, linewidth=linewidth, label=\"valid\")\n",
    "    return train_values[train_values > 0].shape[0]\n",
    "\n",
    "curves_figure = pyplot.figure(figsize = (10, 4))\n",
    "axis = curves_figure.add_subplot(1, 2, 1)\n",
    "\n",
    "new_model_epochs = plot_learning_curves(axis, params, train_column = \"train_accuracy_history\", valid_column = \"valid_accuracy_history\")\n",
    "old_model_epochs = 0 #plot_learning_curves(\"3con_2fc_b36_e1000_aug_lrdec_mominc\", linewidth = 1)\n",
    "\n",
    "pyplot.grid()\n",
    "pyplot.legend()\n",
    "pyplot.xlabel(\"epoch\")\n",
    "pyplot.ylabel(\"accuracy\")\n",
    "pyplot.ylim(50., 102.)\n",
    "pyplot.xlim(0, max(new_model_epochs, old_model_epochs))\n",
    "\n",
    "axis = curves_figure.add_subplot(1, 2, 2)\n",
    "new_model_epochs = plot_learning_curves(axis, model_name, train_column = \"train_loss_history\", valid_column = \"valid_loss_history\")\n",
    "old_model_epochs = 0 #plot_learning_curves(\"3con_2fc_b36_e1000_aug_lrdec_mominc\", linewidth = 1)\n",
    "\n",
    "pyplot.grid()\n",
    "pyplot.legend()\n",
    "pyplot.xlabel(\"epoch\")\n",
    "pyplot.ylabel(\"loss\")\n",
    "pyplot.ylim(0.0001, 10.)\n",
    "pyplot.xlim(0, max(new_model_epochs, old_model_epochs))\n",
    "pyplot.yscale(\"log\")\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "_How did you train your model? (Type of optimizer, batch size, epochs, hyperparameters, etc.)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "\n",
    "_What approach did you take in coming up with a solution to this problem?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "Take several pictures of traffic signs that you find on the web or around you (at least five), and run them through your classifier on your computer to produce example results. The classifier might not recognize some local signs but it could prove interesting nonetheless.\n",
    "\n",
    "You may find `signnames.csv` useful as it contains mappings from the class id (integer) to the actual sign name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load the images and plot them here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "_Choose five candidate images of traffic signs and provide them in the report. Are there any particular qualities of the image(s) that might make classification difficult? It would be helpful to plot the images in the notebook._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Run the predictions here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "_Is your model able to perform equally well on captured pictures when compared to testing on the dataset?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Visualize the softmax probabilities here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "*Use the model's softmax probabilities to visualize the **certainty** of its predictions, [`tf.nn.top_k`](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#top_k) could prove helpful here. Which predictions is the model certain of? Uncertain? If the model was incorrect in its initial prediction, does the correct prediction appear in the top k? (k should be 5 at most)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "_If necessary, provide documentation for how an interface was built for your model to load and classify newly-acquired images._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
